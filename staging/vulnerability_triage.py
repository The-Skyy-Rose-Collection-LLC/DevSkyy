#!/usr/bin/env python3
"""
Vulnerability Triage System
Combines ZAP and Nuclei findings, removes duplicates, prioritizes by severity,
generates unified reports, and identifies blockers vs minor issues.
"""

import json
import sys
from collections import defaultdict
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Optional


class Severity(Enum):
    """Severity levels"""

    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class RemediationPriority(Enum):
    """Remediation priority levels"""

    BLOCKER = "blocker"  # Must fix before production
    URGENT = "urgent"  # Fix within 24-48 hours
    HIGH = "high"  # Fix within 1 week
    MEDIUM = "medium"  # Fix within 2 weeks
    LOW = "low"  # Fix when possible
    BACKLOG = "backlog"  # Track but not urgent


@dataclass
class UnifiedVulnerability:
    """Unified vulnerability from multiple sources"""

    id: str
    title: str
    severity: str
    priority: str
    sources: list[str]  # ["zap", "nuclei"]
    vulnerability_type: str
    description: str
    affected_urls: list[str]
    evidence: list[dict[str, Any]]
    cve_id: Optional[str]
    cwe_id: Optional[str]
    owasp_category: Optional[str]
    cvss_score: Optional[float]
    remediation: str
    remediation_complexity: str  # "low", "medium", "high"
    estimated_effort: str  # "1h", "4h", "1d", "3d", "1w"
    is_false_positive: bool
    false_positive_reason: Optional[str]
    is_duplicate: bool
    duplicate_of: Optional[str]
    tags: list[str]


class VulnerabilityTriage:
    """Triage and prioritize vulnerabilities from multiple sources"""

    # Blockers - must fix before production
    BLOCKER_CRITERIA = [
        {"severity": "critical", "types": ["rce", "injection", "authentication"]},
        {"cve_cvss": 9.0},
        {"keywords": ["remote code execution", "SQL injection", "authentication bypass"]},
    ]

    # Remediation complexity estimation
    COMPLEXITY_PATTERNS = {
        "low": [
            "header",
            "cookie",
            "configuration",
            "version disclosure",
        ],
        "medium": [
            "xss",
            "csrf",
            "misconfiguration",
            "exposure",
        ],
        "high": [
            "injection",
            "authentication",
            "authorization",
            "cryptographic",
            "deserialization",
        ],
    }

    # Effort estimation (simplified)
    EFFORT_MAPPING = {
        "low": "1-2 hours",
        "medium": "4-8 hours",
        "high": "1-3 days",
    }

    def __init__(self):
        self.vulnerabilities: list[UnifiedVulnerability] = []
        self.statistics: dict[str, Any] = {}
        self.blockers: list[UnifiedVulnerability] = []

    def load_zap_findings(self, zap_path: Path) -> list[dict[str, Any]]:
        """Load parsed ZAP findings"""
        if not zap_path.exists():
            print(f"Warning: ZAP report not found: {zap_path}")
            return []

        with open(zap_path) as f:
            data = json.load(f)

        return data.get("findings", [])

    def load_nuclei_findings(self, nuclei_path: Path) -> list[dict[str, Any]]:
        """Load parsed Nuclei findings"""
        if not nuclei_path.exists():
            print(f"Warning: Nuclei report not found: {nuclei_path}")
            return []

        with open(nuclei_path) as f:
            data = json.load(f)

        return data.get("findings", [])

    def normalize_title(self, title: str) -> str:
        """Normalize vulnerability title for comparison"""
        import re

        # Remove version numbers, dates, and common variations
        normalized = re.sub(r"\d+\.\d+\.\d+", "", title)
        normalized = re.sub(r"\d{4}-\d{2}-\d{2}", "", normalized)
        normalized = normalized.lower().strip()
        return normalized

    def find_duplicates(
        self, vuln: dict[str, Any], existing: list[UnifiedVulnerability]
    ) -> Optional[str]:
        """Find if vulnerability is duplicate of existing one"""
        vuln_title = self.normalize_title(vuln.get("title", ""))
        vuln_urls = set()

        # Extract URLs from different sources
        if "instances" in vuln:  # ZAP
            vuln_urls = {inst.get("url", "") for inst in vuln["instances"]}
        elif "matched_at" in vuln:  # Nuclei
            vuln_urls = {vuln["matched_at"]}

        for existing_vuln in existing:
            existing_title = self.normalize_title(existing_vuln.title)

            # Check title similarity
            title_match = vuln_title in existing_title or existing_title in vuln_title

            # Check URL overlap
            existing_urls = set(existing_vuln.affected_urls)
            url_overlap = bool(vuln_urls & existing_urls)

            # Check CVE match
            cve_match = vuln.get("cve_id") and vuln.get("cve_id") == existing_vuln.cve_id

            if title_match and (url_overlap or cve_match):
                return existing_vuln.id

        return None

    def determine_priority(self, vuln: UnifiedVulnerability) -> str:
        """Determine remediation priority"""
        # Check blocker criteria
        for criteria in self.BLOCKER_CRITERIA:
            if "severity" in criteria and "types" in criteria:
                if (
                    vuln.severity == criteria["severity"]
                    and vuln.vulnerability_type in criteria["types"]
                ):
                    return RemediationPriority.BLOCKER.value

            if "cve_cvss" in criteria:
                if vuln.cvss_score and vuln.cvss_score >= criteria["cve_cvss"]:
                    return RemediationPriority.BLOCKER.value

            if "keywords" in criteria:
                desc_lower = vuln.description.lower()
                if any(kw in desc_lower for kw in criteria["keywords"]):
                    return RemediationPriority.BLOCKER.value

        # Map severity to priority
        severity_priority = {
            Severity.CRITICAL.value: RemediationPriority.URGENT.value,
            Severity.HIGH.value: RemediationPriority.HIGH.value,
            Severity.MEDIUM.value: RemediationPriority.MEDIUM.value,
            Severity.LOW.value: RemediationPriority.LOW.value,
            Severity.INFO.value: RemediationPriority.BACKLOG.value,
        }

        return severity_priority.get(vuln.severity, RemediationPriority.MEDIUM.value)

    def estimate_complexity(self, title: str, description: str, vuln_type: str) -> str:
        """Estimate remediation complexity"""
        text = (title + " " + description + " " + vuln_type).lower()

        for complexity, patterns in self.COMPLEXITY_PATTERNS.items():
            if any(pattern in text for pattern in patterns):
                return complexity

        return "medium"

    def convert_zap_to_unified(self, zap_finding: dict[str, Any]) -> UnifiedVulnerability:
        """Convert ZAP finding to unified format"""
        import hashlib

        # Generate unique ID
        id_source = f"zap-{zap_finding['id']}-{zap_finding['title']}"
        vuln_id = hashlib.sha256(id_source.encode()).hexdigest()[:16]

        affected_urls = [inst.get("url", "") for inst in zap_finding.get("instances", [])]
        evidence = [
            {
                "source": "zap",
                "url": inst.get("url", ""),
                "method": inst.get("method", ""),
                "parameter": inst.get("param", ""),
                "evidence": inst.get("evidence", ""),
            }
            for inst in zap_finding.get("instances", [])
        ]

        complexity = self.estimate_complexity(
            zap_finding.get("title", ""),
            zap_finding.get("description", ""),
            zap_finding.get("owasp_category", ""),
        )

        vuln = UnifiedVulnerability(
            id=vuln_id,
            title=zap_finding.get("title", "Unknown"),
            severity=zap_finding.get("severity", "info"),
            priority="",  # Will be set later
            sources=["zap"],
            vulnerability_type="web-application",
            description=zap_finding.get("description", ""),
            affected_urls=affected_urls,
            evidence=evidence,
            cve_id=None,  # ZAP doesn't typically provide CVE
            cwe_id=zap_finding.get("cwe_id"),
            owasp_category=zap_finding.get("owasp_category"),
            cvss_score=None,
            remediation=zap_finding.get("solution", ""),
            remediation_complexity=complexity,
            estimated_effort=self.EFFORT_MAPPING.get(complexity, "Unknown"),
            is_false_positive=zap_finding.get("is_false_positive", False),
            false_positive_reason=zap_finding.get("false_positive_reason"),
            is_duplicate=False,
            duplicate_of=None,
            tags=["web", "zap"],
        )

        vuln.priority = self.determine_priority(vuln)
        return vuln

    def convert_nuclei_to_unified(self, nuclei_finding: dict[str, Any]) -> UnifiedVulnerability:
        """Convert Nuclei finding to unified format"""
        import hashlib

        # Generate unique ID
        id_source = f"nuclei-{nuclei_finding['template_id']}-{nuclei_finding['name']}"
        vuln_id = hashlib.sha256(id_source.encode()).hexdigest()[:16]

        affected_urls = [nuclei_finding.get("matched_at", "")]
        evidence = [
            {
                "source": "nuclei",
                "template": nuclei_finding.get("template_id", ""),
                "matcher": nuclei_finding.get("matcher_name", ""),
                "extracted": nuclei_finding.get("extracted_results", []),
                "curl": nuclei_finding.get("curl_command", ""),
            }
        ]

        complexity = self.estimate_complexity(
            nuclei_finding.get("name", ""),
            nuclei_finding.get("description", ""),
            nuclei_finding.get("vulnerability_type", ""),
        )

        tags = nuclei_finding.get("tags", []) + ["nuclei"]
        if nuclei_finding.get("cve_id"):
            tags.append("cve")

        vuln = UnifiedVulnerability(
            id=vuln_id,
            title=nuclei_finding.get("name", "Unknown"),
            severity=nuclei_finding.get("severity", "info"),
            priority="",  # Will be set later
            sources=["nuclei"],
            vulnerability_type=nuclei_finding.get("vulnerability_type", "other"),
            description=nuclei_finding.get("description", ""),
            affected_urls=affected_urls,
            evidence=evidence,
            cve_id=nuclei_finding.get("cve_id"),
            cwe_id=nuclei_finding.get("cwe_id"),
            owasp_category=None,
            cvss_score=nuclei_finding.get("cvss_score"),
            remediation=nuclei_finding.get("remediation", ""),
            remediation_complexity=complexity,
            estimated_effort=self.EFFORT_MAPPING.get(complexity, "Unknown"),
            is_false_positive=nuclei_finding.get("is_false_positive", False),
            false_positive_reason=nuclei_finding.get("false_positive_reason"),
            is_duplicate=False,
            duplicate_of=None,
            tags=tags,
        )

        vuln.priority = self.determine_priority(vuln)
        return vuln

    def merge_duplicates(
        self, existing: UnifiedVulnerability, new: UnifiedVulnerability
    ) -> UnifiedVulnerability:
        """Merge duplicate vulnerability into existing one"""
        # Combine sources
        existing.sources = list(set(existing.sources + new.sources))

        # Combine URLs
        existing.affected_urls = list(set(existing.affected_urls + new.affected_urls))

        # Combine evidence
        existing.evidence.extend(new.evidence)

        # Use more severe severity
        severity_order = ["critical", "high", "medium", "low", "info"]
        if severity_order.index(new.severity) < severity_order.index(existing.severity):
            existing.severity = new.severity
            existing.priority = self.determine_priority(existing)

        # Prefer CVE/CWE info if available
        if new.cve_id and not existing.cve_id:
            existing.cve_id = new.cve_id
        if new.cwe_id and not existing.cwe_id:
            existing.cwe_id = new.cwe_id
        if new.cvss_score and not existing.cvss_score:
            existing.cvss_score = new.cvss_score

        # Combine tags
        existing.tags = list(set(existing.tags + new.tags))

        return existing

    def triage(self, zap_path: Path, nuclei_path: Path) -> None:
        """Triage vulnerabilities from both sources"""
        print("Loading ZAP findings...")
        zap_findings = self.load_zap_findings(zap_path)
        print(f"Loaded {len(zap_findings)} ZAP findings")

        print("Loading Nuclei findings...")
        nuclei_findings = self.load_nuclei_findings(nuclei_path)
        print(f"Loaded {len(nuclei_findings)} Nuclei findings")

        # Process ZAP findings
        print("\nProcessing ZAP findings...")
        for zap_finding in zap_findings:
            if zap_finding.get("is_false_positive"):
                continue

            unified = self.convert_zap_to_unified(zap_finding)

            # Check for duplicates
            duplicate_id = self.find_duplicates(zap_finding, self.vulnerabilities)
            if duplicate_id:
                # Merge with existing
                existing_idx = next(
                    i for i, v in enumerate(self.vulnerabilities) if v.id == duplicate_id
                )
                self.vulnerabilities[existing_idx] = self.merge_duplicates(
                    self.vulnerabilities[existing_idx], unified
                )
                unified.is_duplicate = True
                unified.duplicate_of = duplicate_id
            else:
                self.vulnerabilities.append(unified)

        # Process Nuclei findings
        print("Processing Nuclei findings...")
        for nuclei_finding in nuclei_findings:
            if nuclei_finding.get("is_false_positive"):
                continue

            unified = self.convert_nuclei_to_unified(nuclei_finding)

            # Check for duplicates
            duplicate_id = self.find_duplicates(nuclei_finding, self.vulnerabilities)
            if duplicate_id:
                # Merge with existing
                existing_idx = next(
                    i for i, v in enumerate(self.vulnerabilities) if v.id == duplicate_id
                )
                self.vulnerabilities[existing_idx] = self.merge_duplicates(
                    self.vulnerabilities[existing_idx], unified
                )
                unified.is_duplicate = True
                unified.duplicate_of = duplicate_id
            else:
                self.vulnerabilities.append(unified)

        # Sort by priority and severity
        priority_order = ["blocker", "urgent", "high", "medium", "low", "backlog"]
        severity_order = ["critical", "high", "medium", "low", "info"]

        self.vulnerabilities.sort(
            key=lambda v: (priority_order.index(v.priority), severity_order.index(v.severity))
        )

        # Identify blockers
        self.blockers = [
            v for v in self.vulnerabilities if v.priority == RemediationPriority.BLOCKER.value
        ]

        self.calculate_statistics()

    def calculate_statistics(self) -> None:
        """Calculate triage statistics"""
        by_severity = defaultdict(int)
        by_priority = defaultdict(int)
        by_type = defaultdict(int)
        by_source = defaultdict(int)

        duplicates_found = 0
        multi_source = 0

        for vuln in self.vulnerabilities:
            by_severity[vuln.severity] += 1
            by_priority[vuln.priority] += 1
            by_type[vuln.vulnerability_type] += 1

            for source in vuln.sources:
                by_source[source] += 1

            if vuln.is_duplicate:
                duplicates_found += 1

            if len(vuln.sources) > 1:
                multi_source += 1

        self.statistics = {
            "total_vulnerabilities": len(self.vulnerabilities),
            "unique_vulnerabilities": len([v for v in self.vulnerabilities if not v.is_duplicate]),
            "duplicates_removed": duplicates_found,
            "multi_source_findings": multi_source,
            "blockers": len(self.blockers),
            "by_severity": dict(by_severity),
            "by_priority": dict(by_priority),
            "by_type": dict(by_type),
            "by_source": dict(by_source),
        }

    def generate_remediation_plan(self) -> dict[str, Any]:
        """Generate remediation plan"""
        plan = {
            "immediate_action": [],
            "short_term": [],
            "medium_term": [],
            "long_term": [],
        }

        for vuln in self.vulnerabilities:
            if vuln.is_duplicate:
                continue

            item = {
                "id": vuln.id,
                "title": vuln.title,
                "severity": vuln.severity,
                "priority": vuln.priority,
                "complexity": vuln.remediation_complexity,
                "estimated_effort": vuln.estimated_effort,
                "remediation": vuln.remediation,
            }

            if vuln.priority == RemediationPriority.BLOCKER.value:
                plan["immediate_action"].append(item)
            elif vuln.priority in [
                RemediationPriority.URGENT.value,
                RemediationPriority.HIGH.value,
            ]:
                plan["short_term"].append(item)
            elif vuln.priority == RemediationPriority.MEDIUM.value:
                plan["medium_term"].append(item)
            else:
                plan["long_term"].append(item)

        return plan

    def export_json(self, output_path: Path) -> None:
        """Export triage results to JSON"""
        remediation_plan = self.generate_remediation_plan()

        output = {
            "metadata": {
                "tool": "VulnerabilityTriage",
                "version": "1.0.0",
                "generated_at": datetime.now().isoformat(),
            },
            "statistics": self.statistics,
            "blockers": [asdict(v) for v in self.blockers],
            "vulnerabilities": [asdict(v) for v in self.vulnerabilities if not v.is_duplicate],
            "duplicates": [asdict(v) for v in self.vulnerabilities if v.is_duplicate],
            "remediation_plan": remediation_plan,
        }

        with open(output_path, "w") as f:
            json.dump(output, f, indent=2)

        print(f"\nExported triage report: {output_path}")

    def print_summary(self) -> None:
        """Print triage summary"""
        print("\n" + "=" * 80)
        print("VULNERABILITY TRIAGE SUMMARY")
        print("=" * 80)
        print(f"Total Vulnerabilities: {self.statistics['total_vulnerabilities']}")
        print(f"Unique Vulnerabilities: {self.statistics['unique_vulnerabilities']}")
        print(f"Duplicates Removed: {self.statistics['duplicates_removed']}")
        print(f"Multi-Source Findings: {self.statistics['multi_source_findings']}")
        print(f"\nPRODUCTION BLOCKERS: {self.statistics['blockers']}")

        if self.blockers:
            print("\nBLOCKER DETAILS:")
            print("-" * 80)
            for blocker in self.blockers:
                print(f"  [{blocker.severity.upper()}] {blocker.title}")
                print(f"    Type: {blocker.vulnerability_type}")
                print(f"    Sources: {', '.join(blocker.sources)}")
                if blocker.cve_id:
                    print(f"    CVE: {blocker.cve_id}")
                print(f"    Remediation: {blocker.remediation[:100]}...")
                print()

        print("\nSEVERITY DISTRIBUTION:")
        for severity in ["critical", "high", "medium", "low", "info"]:
            count = self.statistics["by_severity"].get(severity, 0)
            print(f"  {severity.upper():12} {count:3}")

        print("\nPRIORITY DISTRIBUTION:")
        for priority in ["blocker", "urgent", "high", "medium", "low", "backlog"]:
            count = self.statistics["by_priority"].get(priority, 0)
            print(f"  {priority.upper():12} {count:3}")


def main():
    """Main entry point"""
    if len(sys.argv) < 3:
        print(
            "Usage: python vulnerability_triage.py <zap_parsed.json> <nuclei_parsed.json> [output.json]"
        )
        sys.exit(1)

    zap_path = Path(sys.argv[1])
    nuclei_path = Path(sys.argv[2])
    output_path = Path(sys.argv[3]) if len(sys.argv) > 3 else Path("vulnerability_triage.json")

    triage = VulnerabilityTriage()
    triage.triage(zap_path, nuclei_path)
    triage.export_json(output_path)
    triage.print_summary()

    # Exit with error if blockers found
    if triage.blockers:
        print("\n" + "=" * 80)
        print("WARNING: Production blockers found!")
        print("=" * 80)
        sys.exit(1)
    else:
        print("\n" + "=" * 80)
        print("No production blockers - safe to proceed")
        print("=" * 80)
        sys.exit(0)


if __name__ == "__main__":
    main()
