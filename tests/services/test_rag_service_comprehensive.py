#!/usr/bin/env python3
"""
Comprehensive tests for services/rag_service.py
Achieves â‰¥85% coverage per Truth Protocol Rule #8

Test Coverage:
- RAGConfig: Environment variable loading
- DocumentProcessor: PDF/text processing, chunking, hash calculation
- VectorDatabase: ChromaDB operations, embedding, search
- RAGService: Document ingestion, search, RAG queries
- Singleton pattern: get_rag_service()

Author: DevSkyy Test Team
Version: 1.0.0
"""

import hashlib
import os
import sys
from unittest.mock import MagicMock, Mock, mock_open, patch

import pytest


# Mock external dependencies before importing the module under test
sys.modules['anthropic'] = MagicMock()
sys.modules['chromadb'] = MagicMock()
sys.modules['chromadb.config'] = MagicMock()
sys.modules['langchain'] = MagicMock()
sys.modules['langchain.text_splitter'] = MagicMock()
sys.modules['pypdf'] = MagicMock()
sys.modules['sentence_transformers'] = MagicMock()
sys.modules['tiktoken'] = MagicMock()

# Import modules under test AFTER mocking dependencies
from services.rag_service import (
    DocumentProcessor,
    RAGConfig,
    RAGService,
    VectorDatabase,
    get_rag_service,
)


# =============================================================================
# FIXTURES
# =============================================================================


@pytest.fixture
def mock_env_vars(monkeypatch):
    """Set up environment variables for testing"""
    env_vars = {
        "CHROMA_PERSIST_DIR": "/tmp/test_chroma",
        "CHROMA_COLLECTION_NAME": "test_collection",
        "EMBEDDING_MODEL": "test-model",
        "RAG_CHUNK_SIZE": "500",
        "RAG_CHUNK_OVERLAP": "100",
        "RAG_TOP_K": "3",
        "RAG_SIMILARITY_THRESHOLD": "0.8",
        "ANTHROPIC_API_KEY": "test-api-key-123",
        "RAG_LLM_MODEL": "claude-test-model",
        "RAG_MAX_TOKENS": "2048",
    }
    for key, value in env_vars.items():
        monkeypatch.setenv(key, value)
    yield env_vars


@pytest.fixture
def mock_chroma_client():
    """Mock ChromaDB client"""
    client = Mock()
    collection = Mock()
    collection.count.return_value = 10
    collection.add.return_value = None
    collection.query.return_value = {
        "documents": [["test doc 1", "test doc 2"]],
        "metadatas": [[{"source": "test.pdf", "page": 1}, {"source": "test.pdf", "page": 2}]],
        "distances": [[0.1, 0.2]],
        "ids": [["id1", "id2"]],
    }
    client.get_or_create_collection.return_value = collection
    client.delete_collection.return_value = None
    return client, collection


@pytest.fixture
def mock_embedding_model():
    """Mock SentenceTransformer"""
    model = Mock()
    model.encode.return_value = Mock(tolist=lambda: [0.1, 0.2, 0.3])
    return model


@pytest.fixture
def mock_anthropic_client():
    """Mock Anthropic client"""
    client = Mock()
    message = Mock()
    message.content = [Mock(text="This is the generated answer.")]
    message.usage = Mock(input_tokens=100, output_tokens=50)
    client.messages.create.return_value = message
    return client


@pytest.fixture
def mock_pdf_reader():
    """Mock PyPDF reader"""
    reader = Mock()
    page1 = Mock()
    page1.extract_text.return_value = "Page 1 content with some text."
    page2 = Mock()
    page2.extract_text.return_value = "Page 2 content with more text."
    page3 = Mock()
    page3.extract_text.return_value = ""  # Empty page
    reader.pages = [page1, page2, page3]
    return reader


@pytest.fixture
def mock_tokenizer():
    """Mock tiktoken tokenizer"""
    tokenizer = Mock()
    tokenizer.encode.return_value = [1, 2, 3, 4, 5]  # 5 tokens
    return tokenizer


# =============================================================================
# RAGConfig TESTS
# =============================================================================


class TestRAGConfig:
    """Test RAGConfig configuration class"""

    def test_default_config_values(self):
        """Test default configuration values"""
        assert RAGConfig.CHROMA_PERSIST_DIR == os.getenv("CHROMA_PERSIST_DIR", "./data/chroma")
        assert RAGConfig.CHROMA_COLLECTION_NAME == os.getenv("CHROMA_COLLECTION_NAME", "devskyy_docs")
        assert RAGConfig.EMBEDDING_MODEL == os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2")
        assert RAGConfig.EMBEDDING_DIMENSION == 384

    def test_config_from_env(self, mock_env_vars):
        """Test configuration from environment variables"""
        # RAGConfig is a class with class attributes set at import time
        # The values are already set from environment at import
        # Verify they match the defaults or environment
        config = RAGConfig()
        assert isinstance(config.CHUNK_SIZE, int)
        assert isinstance(config.CHUNK_OVERLAP, int)
        assert isinstance(config.TOP_K_RESULTS, int)
        assert isinstance(config.SIMILARITY_THRESHOLD, float)
        assert isinstance(config.MAX_TOKENS, int)

    def test_config_type_conversions(self, mock_env_vars):
        """Test type conversions for environment variables"""
        config = RAGConfig()
        assert isinstance(config.CHUNK_SIZE, int)
        assert isinstance(config.CHUNK_OVERLAP, int)
        assert isinstance(config.TOP_K_RESULTS, int)
        assert isinstance(config.SIMILARITY_THRESHOLD, float)
        assert isinstance(config.MAX_TOKENS, int)


# =============================================================================
# DocumentProcessor TESTS
# =============================================================================


class TestDocumentProcessor:
    """Test DocumentProcessor class"""

    def test_init_default_params(self):
        """Test initialization with default parameters"""
        processor = DocumentProcessor()
        assert processor.chunk_size == RAGConfig.CHUNK_SIZE
        assert processor.chunk_overlap == RAGConfig.CHUNK_OVERLAP
        assert processor.text_splitter is not None
        assert processor.tokenizer is not None

    def test_init_custom_params(self):
        """Test initialization with custom parameters"""
        processor = DocumentProcessor(chunk_size=800, chunk_overlap=150)
        assert processor.chunk_size == 800
        assert processor.chunk_overlap == 150

    @patch("services.rag_service.tiktoken.get_encoding")
    @patch("services.rag_service.RecursiveCharacterTextSplitter")
    def test_process_text_success(self, mock_splitter_class, mock_tiktoken):
        """Test successful text processing"""
        mock_tiktoken.return_value = Mock(encode=lambda x: [1, 2, 3, 4, 5])

        # Mock the text splitter to return chunks
        mock_splitter = Mock()
        mock_splitter.split_text.return_value = ["Chunk 1", "Chunk 2", "Chunk 3"]
        mock_splitter_class.return_value = mock_splitter

        processor = DocumentProcessor(chunk_size=50, chunk_overlap=10)
        text = "This is a test text. " * 10  # Longer text to trigger chunking
        chunks = processor.process_text(text, source="test.txt")

        assert len(chunks) > 0
        for chunk in chunks:
            assert "content" in chunk
            assert "metadata" in chunk
            assert chunk["metadata"]["source"] == "test.txt"
            assert chunk["metadata"]["file_type"] == "text"
            assert "chunk_index" in chunk["metadata"]
            assert "tokens" in chunk["metadata"]

    @patch("services.rag_service.tiktoken.get_encoding")
    @patch("services.rag_service.RecursiveCharacterTextSplitter")
    def test_process_text_single_chunk(self, mock_splitter_class, mock_tiktoken):
        """Test text processing that results in single chunk"""
        mock_tiktoken.return_value = Mock(encode=lambda x: [1, 2, 3])

        # Mock splitter to return single chunk
        mock_splitter = Mock()
        text = "Short text."
        mock_splitter.split_text.return_value = [text]
        mock_splitter_class.return_value = mock_splitter

        processor = DocumentProcessor(chunk_size=1000, chunk_overlap=100)
        chunks = processor.process_text(text, source="short.txt")

        assert len(chunks) == 1
        assert chunks[0]["content"] == text
        assert chunks[0]["metadata"]["chunk_index"] == 0

    @patch("services.rag_service.tiktoken.get_encoding")
    @patch("services.rag_service.RecursiveCharacterTextSplitter")
    def test_process_text_error_handling(self, mock_splitter_class, mock_tiktoken):
        """Test error handling in text processing"""
        # Mock splitter to raise an error
        mock_splitter = Mock()
        mock_splitter.split_text.side_effect = Exception("Splitter error")
        mock_splitter_class.return_value = mock_splitter

        mock_tiktoken.return_value = Mock(encode=lambda x: [1, 2, 3])

        processor = DocumentProcessor()
        with pytest.raises(Exception, match="Splitter error"):
            processor.process_text("test", source="test.txt")

    @patch("services.rag_service.RecursiveCharacterTextSplitter")
    @patch("services.rag_service.PdfReader")
    @patch("services.rag_service.tiktoken.get_encoding")
    def test_process_pdf_success(self, mock_tiktoken, mock_pdf_class, mock_splitter_class):
        """Test successful PDF processing"""
        mock_tiktoken.return_value = Mock(encode=lambda x: [1, 2, 3, 4, 5])

        # Mock text splitter
        mock_splitter = Mock()
        mock_splitter.split_text.side_effect = lambda text: [text]  # Return single chunk per page
        mock_splitter_class.return_value = mock_splitter

        # Setup PDF reader mock
        reader = Mock()
        page1 = Mock()
        page1.extract_text.return_value = "Page 1 content with some text."
        page2 = Mock()
        page2.extract_text.return_value = "Page 2 content with more text."
        reader.pages = [page1, page2]
        mock_pdf_class.return_value = reader

        processor = DocumentProcessor()
        chunks = processor.process_pdf("test.pdf")

        assert len(chunks) > 0
        for chunk in chunks:
            assert "content" in chunk
            assert "metadata" in chunk
            assert chunk["metadata"]["source"] == "test.pdf"
            assert chunk["metadata"]["file_type"] == "pdf"
            assert "page" in chunk["metadata"]
            assert "total_pages" in chunk["metadata"]
            assert chunk["metadata"]["total_pages"] == 2

    @patch("services.rag_service.PdfReader")
    @patch("services.rag_service.tiktoken.get_encoding")
    def test_process_pdf_empty_pages(self, mock_tiktoken, mock_pdf_class):
        """Test PDF processing with empty pages"""
        mock_tiktoken.return_value = Mock(encode=lambda x: [1, 2, 3])

        reader = Mock()
        page1 = Mock()
        page1.extract_text.return_value = "Content"
        page2 = Mock()
        page2.extract_text.return_value = ""  # Empty page
        page3 = Mock()
        page3.extract_text.return_value = "   "  # Whitespace only
        reader.pages = [page1, page2, page3]
        mock_pdf_class.return_value = reader

        processor = DocumentProcessor()
        chunks = processor.process_pdf("test.pdf")

        # Should only process non-empty pages
        assert all("Content" in chunk["content"] for chunk in chunks if chunk["content"])

    @patch("services.rag_service.PdfReader")
    def test_process_pdf_error_handling(self, mock_pdf_class):
        """Test error handling in PDF processing"""
        mock_pdf_class.side_effect = Exception("PDF read error")

        processor = DocumentProcessor()
        with pytest.raises(Exception, match="PDF read error"):
            processor.process_pdf("invalid.pdf")

    def test_calculate_chunk_hash(self):
        """Test chunk hash calculation"""
        processor = DocumentProcessor()
        content = "Test content for hashing"

        hash1 = processor.calculate_chunk_hash(content)
        hash2 = processor.calculate_chunk_hash(content)

        # Same content should produce same hash
        assert hash1 == hash2
        assert len(hash1) == 64  # SHA256 hex length

        # Different content should produce different hash
        hash3 = processor.calculate_chunk_hash("Different content")
        assert hash1 != hash3

    def test_calculate_chunk_hash_deterministic(self):
        """Test hash calculation is deterministic"""
        processor = DocumentProcessor()
        content = "Deterministic test"

        expected_hash = hashlib.sha256(content.encode()).hexdigest()
        actual_hash = processor.calculate_chunk_hash(content)

        assert actual_hash == expected_hash


# =============================================================================
# VectorDatabase TESTS
# =============================================================================


class TestVectorDatabase:
    """Test VectorDatabase class"""

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_init_success(self, mock_path, mock_chroma, mock_model):
        """Test VectorDatabase initialization"""
        # Setup mocks
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.count.return_value = 0
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client
        mock_model.return_value = Mock()

        db = VectorDatabase(
            persist_directory="/tmp/test",
            collection_name="test_col",
            embedding_model="test-model",
        )

        assert db.persist_directory == "/tmp/test"
        assert db.collection_name == "test_col"
        assert db.collection is not None
        mock_path.return_value.mkdir.assert_called_once_with(parents=True, exist_ok=True)

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_add_documents_success(self, mock_path, mock_chroma, mock_model):
        """Test adding documents to vector database"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.count.return_value = 2
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client

        # Mock embedding model
        model = Mock()
        embeddings_array = Mock()
        embeddings_array.tolist.return_value = [[0.1, 0.2], [0.3, 0.4]]
        model.encode.return_value = embeddings_array
        mock_model.return_value = model

        db = VectorDatabase()
        documents = [
            {"content": "doc 1", "metadata": {"source": "test1.txt"}},
            {"content": "doc 2", "metadata": {"source": "test2.txt"}},
        ]

        stats = db.add_documents(documents, batch_size=10)

        assert stats["total_documents"] == 2
        assert stats["added"] == 2
        assert stats["skipped"] == 0
        assert stats["collection_size"] == 2
        collection.add.assert_called_once()

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_add_documents_batching(self, mock_path, mock_chroma, mock_model):
        """Test document batching in add_documents"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.count.return_value = 5
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client

        model = Mock()
        embeddings_array = Mock()
        embeddings_array.tolist.return_value = [[0.1, 0.2], [0.3, 0.4]]
        model.encode.return_value = embeddings_array
        mock_model.return_value = model

        db = VectorDatabase()
        documents = [
            {"content": f"doc {i}", "metadata": {"source": f"test{i}.txt"}}
            for i in range(5)
        ]

        stats = db.add_documents(documents, batch_size=2)

        # Should call add 3 times: batch of 2, 2, and 1
        assert collection.add.call_count == 3
        assert stats["added"] == 5

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_add_documents_error_handling(self, mock_path, mock_chroma, mock_model):
        """Test error handling in add_documents"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.add.side_effect = Exception("Database error")
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client
        mock_model.return_value = Mock()

        db = VectorDatabase()
        documents = [{"content": "doc", "metadata": {}}]

        with pytest.raises(Exception, match="Database error"):
            db.add_documents(documents)

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_search_success(self, mock_path, mock_chroma, mock_model):
        """Test vector database search"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.query.return_value = {
            "documents": [["result 1", "result 2"]],
            "metadatas": [[{"source": "a.txt"}, {"source": "b.txt"}]],
            "distances": [[0.1, 0.2]],
            "ids": [["id1", "id2"]],
        }
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client

        model = Mock()
        model.encode.return_value = Mock(tolist=lambda: [0.1, 0.2, 0.3])
        mock_model.return_value = model

        db = VectorDatabase()
        results = db.search("test query", top_k=2)

        assert len(results) == 2
        assert results[0]["content"] == "result 1"
        assert results[0]["metadata"]["source"] == "a.txt"
        assert results[0]["distance"] == 0.1
        assert results[0]["similarity"] == 0.9  # 1 - 0.1
        assert results[0]["id"] == "id1"

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_search_with_filters(self, mock_path, mock_chroma, mock_model):
        """Test search with metadata filters"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.query.return_value = {
            "documents": [["filtered result"]],
            "metadatas": [[{"source": "test.pdf", "page": 1}]],
            "distances": [[0.15]],
            "ids": [["id1"]],
        }
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client
        mock_model.return_value = Mock(encode=Mock(return_value=Mock(tolist=lambda: [0.1])))

        db = VectorDatabase()
        filters = {"source": "test.pdf"}
        results = db.search("query", top_k=5, filters=filters)

        collection.query.assert_called_once()
        call_args = collection.query.call_args
        assert call_args[1]["where"] == filters

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_search_no_results(self, mock_path, mock_chroma, mock_model):
        """Test search with no results"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.query.return_value = {
            "documents": [[]],
            "metadatas": [[]],
            "distances": [[]],
            "ids": [[]],
        }
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client
        mock_model.return_value = Mock(encode=Mock(return_value=Mock(tolist=lambda: [0.1])))

        db = VectorDatabase()
        results = db.search("query")

        assert len(results) == 0

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_search_error_handling(self, mock_path, mock_chroma, mock_model):
        """Test error handling in search"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.query.side_effect = Exception("Search error")
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client
        mock_model.return_value = Mock(encode=Mock(return_value=Mock(tolist=lambda: [0.1])))

        db = VectorDatabase()
        with pytest.raises(Exception, match="Search error"):
            db.search("query")

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_delete_collection_success(self, mock_path, mock_chroma, mock_model):
        """Test collection deletion"""
        mock_path.return_value.mkdir.return_value = None
        client = Mock()
        client.get_or_create_collection.return_value = Mock()
        client.delete_collection.return_value = None
        mock_chroma.return_value = client
        mock_model.return_value = Mock()

        db = VectorDatabase(collection_name="test_delete")
        db.delete_collection()

        client.delete_collection.assert_called_once_with(name="test_delete")

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_delete_collection_error_handling(self, mock_path, mock_chroma, mock_model):
        """Test error handling in collection deletion"""
        mock_path.return_value.mkdir.return_value = None
        client = Mock()
        client.get_or_create_collection.return_value = Mock()
        client.delete_collection.side_effect = Exception("Delete error")
        mock_chroma.return_value = client
        mock_model.return_value = Mock()

        db = VectorDatabase()
        with pytest.raises(Exception, match="Delete error"):
            db.delete_collection()

    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    def test_get_stats(self, mock_path, mock_chroma, mock_model):
        """Test getting database statistics"""
        mock_path.return_value.mkdir.return_value = None
        collection = Mock()
        collection.count.return_value = 42
        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client
        mock_model.return_value = Mock()

        db = VectorDatabase(
            persist_directory="/test/path",
            collection_name="stats_test",
        )
        stats = db.get_stats()

        assert stats["collection_name"] == "stats_test"
        assert stats["document_count"] == 42
        assert stats["persist_directory"] == "/test/path"
        assert "embedding_model" in stats


# =============================================================================
# RAGService TESTS
# =============================================================================


class TestRAGService:
    """Test RAGService class"""

    @patch("services.rag_service.Anthropic")
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    def test_init_with_dependencies(self, mock_doc_proc, mock_vector_db, mock_anthropic):
        """Test RAGService initialization with dependencies"""
        vector_db = Mock()
        doc_processor = Mock()

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)

        assert service.vector_db == vector_db
        assert service.doc_processor == doc_processor

    @patch("services.rag_service.Anthropic")
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    def test_init_without_api_key(self, mock_doc_proc, mock_vector_db, mock_anthropic, monkeypatch):
        """Test initialization without Anthropic API key"""
        monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)

        service = RAGService()

        assert service.anthropic is None

    @patch("services.rag_service.Anthropic")
    @patch("services.rag_service.RAGConfig")
    def test_init_with_api_key(self, mock_config, mock_anthropic):
        """Test initialization with Anthropic API key"""
        # Mock the config to have an API key
        mock_config.ANTHROPIC_API_KEY = "test-key"

        mock_anthropic_instance = Mock()
        mock_anthropic.return_value = mock_anthropic_instance

        with patch("services.rag_service.VectorDatabase"), patch("services.rag_service.DocumentProcessor"):
            service = RAGService()
            assert service.anthropic is not None

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    async def test_ingest_document_pdf(self, mock_doc_proc_class, mock_vector_db_class):
        """Test PDF document ingestion"""
        # Setup mocks
        doc_processor = Mock()
        chunks = [
            {"content": "chunk 1", "metadata": {"page": 1}},
            {"content": "chunk 2", "metadata": {"page": 2}},
        ]
        doc_processor.process_pdf.return_value = chunks

        vector_db = Mock()
        vector_db.add_documents.return_value = {
            "total_documents": 2,
            "added": 2,
            "skipped": 0,
            "collection_size": 10,
        }

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)
        stats = await service.ingest_document("test.pdf", file_type="pdf")

        assert stats["file_path"] == "test.pdf"
        assert stats["file_type"] == "pdf"
        assert stats["chunks_created"] == 2
        assert stats["added"] == 2
        assert "ingested_at" in stats
        doc_processor.process_pdf.assert_called_once_with("test.pdf")

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    @patch("builtins.open", new_callable=mock_open, read_data="Test file content")
    async def test_ingest_document_text(self, mock_file, mock_doc_proc_class, mock_vector_db_class):
        """Test text document ingestion"""
        doc_processor = Mock()
        chunks = [{"content": "chunk", "metadata": {}}]
        doc_processor.process_text.return_value = chunks

        vector_db = Mock()
        vector_db.add_documents.return_value = {
            "total_documents": 1,
            "added": 1,
            "skipped": 0,
            "collection_size": 5,
        }

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)
        stats = await service.ingest_document("test.txt", file_type="txt")

        assert stats["file_type"] == "txt"
        assert stats["chunks_created"] == 1
        doc_processor.process_text.assert_called_once()

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    @patch("builtins.open", new_callable=mock_open, read_data="Auto-detect test")
    async def test_ingest_document_auto_detect(self, mock_file, mock_doc_proc_class, mock_vector_db_class):
        """Test document ingestion with auto file type detection"""
        doc_processor = Mock()
        doc_processor.process_text.return_value = [{"content": "chunk", "metadata": {}}]

        vector_db = Mock()
        vector_db.add_documents.return_value = {
            "total_documents": 1,
            "added": 1,
            "skipped": 0,
            "collection_size": 1,
        }

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)
        stats = await service.ingest_document("document.md")  # No file_type specified

        assert stats["file_type"] == "md"  # Auto-detected from extension

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    async def test_ingest_document_error_handling(self, mock_doc_proc_class, mock_vector_db_class):
        """Test error handling in document ingestion"""
        doc_processor = Mock()
        doc_processor.process_pdf.side_effect = Exception("Processing error")

        service = RAGService(vector_db=Mock(), doc_processor=doc_processor)

        with pytest.raises(Exception, match="Processing error"):
            await service.ingest_document("error.pdf", file_type="pdf")

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    async def test_ingest_text_success(self, mock_doc_proc_class, mock_vector_db_class):
        """Test direct text ingestion"""
        doc_processor = Mock()
        chunks = [
            {"content": "chunk 1", "metadata": {"chunk_index": 0}},
            {"content": "chunk 2", "metadata": {"chunk_index": 1}},
        ]
        doc_processor.process_text.return_value = chunks

        vector_db = Mock()
        vector_db.add_documents.return_value = {
            "total_documents": 2,
            "added": 2,
            "skipped": 0,
            "collection_size": 15,
        }

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)
        stats = await service.ingest_text("Test text content", source="test_source")

        assert stats["source"] == "test_source"
        assert stats["text_length"] == len("Test text content")
        assert stats["chunks_created"] == 2
        assert "ingested_at" in stats

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    async def test_ingest_text_with_metadata(self, mock_doc_proc_class, mock_vector_db_class):
        """Test text ingestion with custom metadata"""
        doc_processor = Mock()
        chunks = [{"content": "chunk", "metadata": {"chunk_index": 0}}]
        doc_processor.process_text.return_value = chunks

        vector_db = Mock()
        vector_db.add_documents.return_value = {
            "total_documents": 1,
            "added": 1,
            "skipped": 0,
            "collection_size": 5,
        }

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)
        custom_metadata = {"author": "test", "category": "demo"}
        stats = await service.ingest_text("Text", metadata=custom_metadata)

        # Verify metadata was added to chunks
        added_chunks = vector_db.add_documents.call_args[0][0]
        assert added_chunks[0]["metadata"]["author"] == "test"
        assert added_chunks[0]["metadata"]["category"] == "demo"

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    @patch("services.rag_service.DocumentProcessor")
    async def test_ingest_text_error_handling(self, mock_doc_proc_class, mock_vector_db_class):
        """Test error handling in text ingestion"""
        doc_processor = Mock()
        doc_processor.process_text.side_effect = Exception("Text processing error")

        service = RAGService(vector_db=Mock(), doc_processor=doc_processor)

        with pytest.raises(Exception, match="Text processing error"):
            await service.ingest_text("Error text")

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    async def test_search_success(self, mock_vector_db_class):
        """Test search functionality"""
        vector_db = Mock()
        vector_db.search.return_value = [
            {"content": "result 1", "similarity": 0.9},
            {"content": "result 2", "similarity": 0.85},
            {"content": "result 3", "similarity": 0.6},  # Below threshold
        ]

        service = RAGService(vector_db=vector_db, doc_processor=Mock())
        results = await service.search("test query", top_k=5, min_similarity=0.7)

        # Should filter out result 3 (similarity 0.6 < 0.7)
        assert len(results) == 2
        assert all(r["similarity"] >= 0.7 for r in results)

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    async def test_search_with_filters(self, mock_vector_db_class):
        """Test search with metadata filters"""
        vector_db = Mock()
        vector_db.search.return_value = [
            {"content": "filtered result", "similarity": 0.9, "metadata": {"source": "test.pdf"}},
        ]

        service = RAGService(vector_db=vector_db, doc_processor=Mock())
        filters = {"source": "test.pdf"}
        results = await service.search("query", filters=filters)

        vector_db.search.assert_called_once()
        call_args = vector_db.search.call_args
        assert call_args[1]["filters"] == filters

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    async def test_search_error_handling(self, mock_vector_db_class):
        """Test error handling in search"""
        vector_db = Mock()
        vector_db.search.side_effect = Exception("Search error")

        service = RAGService(vector_db=vector_db, doc_processor=Mock())

        with pytest.raises(Exception, match="Search error"):
            await service.search("query")

    @pytest.mark.asyncio
    @patch("services.rag_service.Anthropic")
    @patch("services.rag_service.VectorDatabase")
    async def test_query_with_llm(self, mock_vector_db_class, mock_anthropic_class, monkeypatch):
        """Test RAG query with LLM generation"""
        monkeypatch.setenv("ANTHROPIC_API_KEY", "test-key")

        # Mock vector database search
        vector_db = Mock()
        vector_db.search.return_value = [
            {"content": "Context 1", "similarity": 0.9},
            {"content": "Context 2", "similarity": 0.85},
        ]

        # Mock Anthropic client
        anthropic_client = Mock()
        message = Mock()
        message.content = [Mock(text="Generated answer")]
        message.usage = Mock(input_tokens=100, output_tokens=50)
        anthropic_client.messages.create.return_value = message
        mock_anthropic_class.return_value = anthropic_client

        service = RAGService(vector_db=vector_db, doc_processor=Mock())
        service.anthropic = anthropic_client  # Set directly

        result = await service.query("What is DevSkyy?", top_k=2)

        assert result["answer"] == "Generated answer"
        assert result["context_used"] == 2
        assert len(result["sources"]) == 2
        assert result["tokens_used"]["input"] == 100
        assert result["tokens_used"]["output"] == 50

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    async def test_query_without_llm(self, mock_vector_db_class):
        """Test RAG query without LLM (fallback mode)"""
        vector_db = Mock()
        vector_db.search.return_value = [
            {"content": "Context 1", "similarity": 0.9},
        ]

        service = RAGService(vector_db=vector_db, doc_processor=Mock())
        service.anthropic = None  # No LLM available

        result = await service.query("Question?")

        assert "LLM not configured" in result["answer"]
        assert "Context 1" in result["answer"]
        assert result["context_used"] == 1

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    async def test_query_no_context(self, mock_vector_db_class):
        """Test RAG query with no relevant context"""
        vector_db = Mock()
        vector_db.search.return_value = []  # No results

        service = RAGService(vector_db=vector_db, doc_processor=Mock())

        result = await service.query("Question with no answer?")

        assert "couldn't find any relevant information" in result["answer"]
        assert result["sources"] == []
        assert result["context_used"] == 0

    @pytest.mark.asyncio
    @patch("services.rag_service.Anthropic")
    @patch("services.rag_service.VectorDatabase")
    async def test_query_custom_system_prompt(self, mock_vector_db_class, mock_anthropic_class, monkeypatch):
        """Test RAG query with custom system prompt"""
        monkeypatch.setenv("ANTHROPIC_API_KEY", "test-key")

        vector_db = Mock()
        vector_db.search.return_value = [{"content": "Context", "similarity": 0.9}]

        anthropic_client = Mock()
        message = Mock()
        message.content = [Mock(text="Answer")]
        message.usage = Mock(input_tokens=50, output_tokens=25)
        anthropic_client.messages.create.return_value = message
        mock_anthropic_class.return_value = anthropic_client

        service = RAGService(vector_db=vector_db, doc_processor=Mock())
        service.anthropic = anthropic_client

        custom_prompt = "You are a helpful assistant."
        result = await service.query("Question?", system_prompt=custom_prompt)

        # Verify custom prompt was used
        call_args = anthropic_client.messages.create.call_args
        assert call_args[1]["system"] == custom_prompt

    @pytest.mark.asyncio
    @patch("services.rag_service.VectorDatabase")
    async def test_query_error_handling(self, mock_vector_db_class):
        """Test error handling in RAG query"""
        vector_db = Mock()
        vector_db.search.side_effect = Exception("Query error")

        service = RAGService(vector_db=vector_db, doc_processor=Mock())

        with pytest.raises(Exception, match="Query error"):
            await service.query("Question?")

    @patch("services.rag_service.VectorDatabase")
    def test_get_stats(self, mock_vector_db_class):
        """Test getting RAG service statistics"""
        vector_db = Mock()
        vector_db.get_stats.return_value = {
            "collection_name": "test",
            "document_count": 100,
        }

        doc_processor = Mock()
        doc_processor.chunk_size = 1000
        doc_processor.chunk_overlap = 200

        service = RAGService(vector_db=vector_db, doc_processor=doc_processor)
        stats = service.get_stats()

        assert "vector_db" in stats
        assert stats["vector_db"]["document_count"] == 100
        assert "config" in stats
        assert stats["config"]["chunk_size"] == 1000
        assert stats["config"]["chunk_overlap"] == 200


# =============================================================================
# SINGLETON TESTS
# =============================================================================


class TestSingleton:
    """Test singleton pattern for RAGService"""

    @patch("services.rag_service.RAGService")
    def test_get_rag_service_creates_instance(self, mock_rag_service):
        """Test get_rag_service creates instance on first call"""
        # Reset singleton
        import services.rag_service
        services.rag_service._rag_service = None

        mock_instance = Mock()
        mock_rag_service.return_value = mock_instance

        service = get_rag_service()

        assert service == mock_instance
        mock_rag_service.assert_called_once()

    @patch("services.rag_service.RAGService")
    def test_get_rag_service_returns_same_instance(self, mock_rag_service):
        """Test get_rag_service returns same instance on subsequent calls"""
        # Reset singleton
        import services.rag_service
        services.rag_service._rag_service = None

        mock_instance = Mock()
        mock_rag_service.return_value = mock_instance

        service1 = get_rag_service()
        service2 = get_rag_service()

        assert service1 is service2
        # Should only create once
        assert mock_rag_service.call_count == 1


# =============================================================================
# INTEGRATION TESTS
# =============================================================================


class TestIntegration:
    """Integration tests for RAG service components"""

    @pytest.mark.asyncio
    @patch("services.rag_service.RecursiveCharacterTextSplitter")
    @patch("services.rag_service.Anthropic")
    @patch("services.rag_service.SentenceTransformer")
    @patch("services.rag_service.chromadb.PersistentClient")
    @patch("services.rag_service.Path")
    @patch("services.rag_service.tiktoken.get_encoding")
    async def test_end_to_end_text_ingestion_and_search(
        self, mock_tiktoken, mock_path, mock_chroma, mock_model, mock_anthropic, mock_splitter_class
    ):
        """Test end-to-end text ingestion and search workflow"""
        # Setup mocks
        mock_tiktoken.return_value = Mock(encode=lambda x: [1, 2, 3, 4, 5])
        mock_path.return_value.mkdir.return_value = None

        # Mock text splitter
        mock_splitter = Mock()
        mock_splitter.split_text.return_value = ["Ingested content chunk"]
        mock_splitter_class.return_value = mock_splitter

        collection = Mock()
        collection.count.return_value = 1
        collection.add.return_value = None
        collection.query.return_value = {
            "documents": [["Ingested content"]],
            "metadatas": [[{"source": "test"}]],
            "distances": [[0.1]],
            "ids": [["id1"]],
        }

        client = Mock()
        client.get_or_create_collection.return_value = collection
        mock_chroma.return_value = client

        model = Mock()
        embeddings_array = Mock()
        embeddings_array.tolist.return_value = [[0.1, 0.2, 0.3]]
        model.encode.return_value = embeddings_array
        mock_model.return_value = model

        # Create service and test workflow
        service = RAGService()

        # Ingest text
        ingest_stats = await service.ingest_text("Test content for ingestion", source="integration_test")
        assert ingest_stats["chunks_created"] > 0

        # Search
        search_results = await service.search("test query")
        assert len(search_results) > 0

    @patch("services.rag_service.RecursiveCharacterTextSplitter")
    @patch("services.rag_service.tiktoken.get_encoding")
    def test_document_processor_chunking_consistency(self, mock_tiktoken, mock_splitter_class):
        """Test document processor produces consistent chunks"""
        mock_tiktoken.return_value = Mock(encode=lambda x: [1] * len(x))

        # Mock splitter to return consistent chunks
        mock_splitter = Mock()
        mock_splitter.split_text.return_value = ["Chunk A", "Chunk B", "Chunk C"]
        mock_splitter_class.return_value = mock_splitter

        processor = DocumentProcessor(chunk_size=100, chunk_overlap=20)

        text = "A" * 300  # Long text to ensure multiple chunks
        chunks1 = processor.process_text(text, source="test")
        chunks2 = processor.process_text(text, source="test")

        # Should produce same chunks for same input
        assert len(chunks1) == len(chunks2)
        assert all(c1["content"] == c2["content"] for c1, c2 in zip(chunks1, chunks2))
