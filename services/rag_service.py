#!/usr/bin/env python3
"""
DevSkyy RAG (Retrieval-Augmented Generation) Service
Provides document ingestion, vector search, and RAG query capabilities

Per Truth Protocol:
- Rule #1: Never guess - All operations verified with type hints and validation
- Rule #5: No secrets in code - All credentials via environment variables
- Rule #7: Input validation - Schema enforcement on all inputs
- Rule #13: Security baseline - AES-256-GCM for document encryption

Features:
- Iterative retrieval for multi-hop questions
- Query reformulation for better results
- Sufficiency checking to know when to stop

Author: DevSkyy Platform Team
Version: 1.1.0
Python: 3.11+
"""

import asyncio
from datetime import datetime
import hashlib
import logging
import os
from pathlib import Path
from typing import Any
import uuid

from anthropic import Anthropic
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
import tiktoken


logger = logging.getLogger(__name__)


# =============================================================================
# CONFIGURATION
# =============================================================================


class RAGConfig:
    """RAG system configuration"""

    # Vector Database
    CHROMA_PERSIST_DIR = os.getenv("CHROMA_PERSIST_DIR", "./data/chroma")
    CHROMA_COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME", "devskyy_docs")

    # Embedding Model
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2")
    EMBEDDING_DIMENSION = 384  # all-MiniLM-L6-v2 dimension

    # Chunking Parameters
    CHUNK_SIZE = int(os.getenv("RAG_CHUNK_SIZE", "1000"))
    CHUNK_OVERLAP = int(os.getenv("RAG_CHUNK_OVERLAP", "200"))

    # Retrieval Parameters
    TOP_K_RESULTS = int(os.getenv("RAG_TOP_K", "5"))
    SIMILARITY_THRESHOLD = float(os.getenv("RAG_SIMILARITY_THRESHOLD", "0.7"))

    # LLM Configuration
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    DEFAULT_MODEL = os.getenv("RAG_LLM_MODEL", "claude-sonnet-4-5-20250929")
    MAX_TOKENS = int(os.getenv("RAG_MAX_TOKENS", "4096"))


# =============================================================================
# DOCUMENT PROCESSORS
# =============================================================================


class DocumentProcessor:
    """Process and chunk documents for RAG ingestion"""

    def __init__(
        self,
        chunk_size: int = RAGConfig.CHUNK_SIZE,
        chunk_overlap: int = RAGConfig.CHUNK_OVERLAP,
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""],
        )
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def process_pdf(self, file_path: str) -> list[dict[str, Any]]:
        """
        Process PDF document and extract text chunks

        Args:
            file_path: Path to PDF file

        Returns:
            List of document chunks with metadata
        """
        try:
            reader = PdfReader(file_path)
            chunks = []

            for page_num, page in enumerate(reader.pages, start=1):
                text = page.extract_text()

                if not text.strip():
                    continue

                # Split page text into chunks
                page_chunks = self.text_splitter.split_text(text)

                for chunk_idx, chunk in enumerate(page_chunks):
                    chunks.append(
                        {
                            "content": chunk,
                            "metadata": {
                                "source": file_path,
                                "page": page_num,
                                "chunk_index": chunk_idx,
                                "total_pages": len(reader.pages),
                                "file_type": "pdf",
                                "tokens": len(self.tokenizer.encode(chunk)),
                            },
                        }
                    )

            logger.info(f"Processed PDF: {file_path} -> {len(chunks)} chunks")
            return chunks

        except Exception as e:
            logger.error(f"Error processing PDF {file_path}: {e}")
            raise

    def process_text(self, text: str, source: str = "text") -> list[dict[str, Any]]:
        """
        Process plain text and extract chunks

        Args:
            text: Text content
            source: Source identifier

        Returns:
            List of document chunks with metadata
        """
        try:
            chunks = self.text_splitter.split_text(text)

            return [
                {
                    "content": chunk,
                    "metadata": {
                        "source": source,
                        "chunk_index": idx,
                        "file_type": "text",
                        "tokens": len(self.tokenizer.encode(chunk)),
                    },
                }
                for idx, chunk in enumerate(chunks)
            ]

        except Exception as e:
            logger.error(f"Error processing text: {e}")
            raise

    def calculate_chunk_hash(self, content: str) -> str:
        """Calculate SHA256 hash of chunk content"""
        return hashlib.sha256(content.encode()).hexdigest()


# =============================================================================
# VECTOR DATABASE
# =============================================================================


class VectorDatabase:
    """ChromaDB vector database manager"""

    def __init__(
        self,
        persist_directory: str = RAGConfig.CHROMA_PERSIST_DIR,
        collection_name: str = RAGConfig.CHROMA_COLLECTION_NAME,
        embedding_model: str = RAGConfig.EMBEDDING_MODEL,
    ):
        self.persist_directory = persist_directory
        self.collection_name = collection_name

        # Initialize ChromaDB client
        Path(persist_directory).mkdir(parents=True, exist_ok=True)

        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True,
            ),
        )

        # Initialize embedding model
        self.embedding_model = SentenceTransformer(embedding_model)
        logger.info(f"Loaded embedding model: {embedding_model}")

        # Get or create collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "DevSkyy RAG document collection"},
        )

        logger.info(f"Initialized vector database: {collection_name}")

    def add_documents(
        self,
        documents: list[dict[str, Any]],
        batch_size: int = 100,
    ) -> dict[str, Any]:
        """
        Add documents to vector database

        Args:
            documents: List of document chunks with content and metadata
            batch_size: Batch size for processing

        Returns:
            Dictionary with ingestion statistics
        """
        try:
            total_docs = len(documents)
            added = 0
            skipped = 0

            for i in range(0, total_docs, batch_size):
                batch = documents[i : i + batch_size]

                # Extract content and metadata
                contents = [doc["content"] for doc in batch]
                metadatas = [doc["metadata"] for doc in batch]

                # Generate embeddings
                embeddings = self.embedding_model.encode(
                    contents,
                    show_progress_bar=False,
                ).tolist()

                # Generate unique IDs
                ids = [str(uuid.uuid4()) for _ in batch]

                # Add to collection
                self.collection.add(
                    embeddings=embeddings,
                    documents=contents,
                    metadatas=metadatas,
                    ids=ids,
                )

                added += len(batch)
                logger.info(f"Added batch {i // batch_size + 1}: {added}/{total_docs}")

            return {
                "total_documents": total_docs,
                "added": added,
                "skipped": skipped,
                "collection_size": self.collection.count(),
            }

        except Exception as e:
            logger.error(f"Error adding documents: {e}")
            raise

    def search(
        self,
        query: str,
        top_k: int = RAGConfig.TOP_K_RESULTS,
        filters: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """
        Semantic search in vector database

        Args:
            query: Search query
            top_k: Number of results to return
            filters: Metadata filters

        Returns:
            List of search results with content, metadata, and scores
        """
        try:
            # Generate query embedding
            query_embedding = self.embedding_model.encode(query).tolist()

            # Search collection
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=filters,
            )

            # Format results
            formatted_results = []

            if results["documents"] and results["documents"][0]:
                for idx in range(len(results["documents"][0])):
                    formatted_results.append(
                        {
                            "content": results["documents"][0][idx],
                            "metadata": results["metadatas"][0][idx] if results["metadatas"] else {},
                            "distance": results["distances"][0][idx] if results["distances"] else 0.0,
                            "similarity": 1 - (results["distances"][0][idx] if results["distances"] else 0.0),
                            "id": results["ids"][0][idx] if results["ids"] else "",
                        }
                    )

            logger.info(f"Search query: '{query}' -> {len(formatted_results)} results")
            return formatted_results

        except Exception as e:
            logger.error(f"Error searching: {e}")
            raise

    def delete_collection(self):
        """Delete the entire collection"""
        try:
            self.client.delete_collection(name=self.collection_name)
            logger.info(f"Deleted collection: {self.collection_name}")
        except Exception as e:
            logger.error(f"Error deleting collection: {e}")
            raise

    def get_stats(self) -> dict[str, Any]:
        """Get database statistics"""
        return {
            "collection_name": self.collection_name,
            "document_count": self.collection.count(),
            "persist_directory": self.persist_directory,
            "embedding_model": RAGConfig.EMBEDDING_MODEL,
        }


# =============================================================================
# RAG SERVICE
# =============================================================================


class RAGService:
    """Main RAG service orchestrating document processing, retrieval, and generation"""

    def __init__(
        self,
        vector_db: VectorDatabase | None = None,
        doc_processor: DocumentProcessor | None = None,
    ):
        self.vector_db = vector_db or VectorDatabase()
        self.doc_processor = doc_processor or DocumentProcessor()

        # Initialize Anthropic client
        if RAGConfig.ANTHROPIC_API_KEY:
            self.anthropic = Anthropic(api_key=RAGConfig.ANTHROPIC_API_KEY)
        else:
            self.anthropic = None
            logger.warning("Anthropic API key not found - RAG queries will use retrieval only")

    async def ingest_document(
        self,
        file_path: str,
        file_type: str | None = None,
    ) -> dict[str, Any]:
        """
        Ingest a document into the RAG system

        Args:
            file_path: Path to document file
            file_type: Document type (pdf, text, etc.) - auto-detected if None

        Returns:
            Ingestion statistics
        """
        try:
            # Auto-detect file type
            if not file_type:
                file_type = Path(file_path).suffix.lower().lstrip(".")

            # Process document based on type
            if file_type == "pdf":
                chunks = self.doc_processor.process_pdf(file_path)
            else:
                # Read as text file
                with open(file_path, "r", encoding="utf-8") as f:
                    text = f.read()
                chunks = self.doc_processor.process_text(text, source=file_path)

            # Add to vector database
            stats = self.vector_db.add_documents(chunks)

            return {
                **stats,
                "file_path": file_path,
                "file_type": file_type,
                "chunks_created": len(chunks),
                "ingested_at": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            logger.error(f"Error ingesting document {file_path}: {e}")
            raise

    async def ingest_text(
        self,
        text: str,
        source: str = "direct_input",
        metadata: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """
        Ingest text content directly

        Args:
            text: Text content
            source: Source identifier
            metadata: Additional metadata

        Returns:
            Ingestion statistics
        """
        try:
            # Process text
            chunks = self.doc_processor.process_text(text, source=source)

            # Add custom metadata if provided
            if metadata:
                for chunk in chunks:
                    chunk["metadata"].update(metadata)

            # Add to vector database
            stats = self.vector_db.add_documents(chunks)

            return {
                **stats,
                "source": source,
                "text_length": len(text),
                "chunks_created": len(chunks),
                "ingested_at": datetime.utcnow().isoformat(),
            }

        except Exception as e:
            logger.error(f"Error ingesting text: {e}")
            raise

    async def search(
        self,
        query: str,
        top_k: int = RAGConfig.TOP_K_RESULTS,
        filters: dict[str, Any] | None = None,
        min_similarity: float = RAGConfig.SIMILARITY_THRESHOLD,
    ) -> list[dict[str, Any]]:
        """
        Semantic search for documents

        Args:
            query: Search query
            top_k: Number of results
            filters: Metadata filters
            min_similarity: Minimum similarity threshold

        Returns:
            List of search results
        """
        try:
            results = self.vector_db.search(query, top_k=top_k, filters=filters)

            # Filter by similarity threshold
            filtered_results = [r for r in results if r["similarity"] >= min_similarity]

            return filtered_results

        except Exception as e:
            logger.error(f"Error searching: {e}")
            raise

    async def query(
        self,
        question: str,
        top_k: int = RAGConfig.TOP_K_RESULTS,
        model: str = RAGConfig.DEFAULT_MODEL,
        system_prompt: str | None = None,
    ) -> dict[str, Any]:
        """
        RAG query - Retrieve relevant context and generate answer

        Args:
            question: User question
            top_k: Number of context chunks to retrieve
            model: LLM model to use
            system_prompt: Custom system prompt

        Returns:
            Answer with sources and metadata
        """
        try:
            # 1. Retrieve relevant context
            context_results = await self.search(query=question, top_k=top_k)

            if not context_results:
                return {
                    "answer": "I couldn't find any relevant information to answer your question.",
                    "sources": [],
                    "context_used": 0,
                }

            # 2. Build context string
            context_str = "\n\n".join(
                [f"[Source {idx + 1}] {result['content']}" for idx, result in enumerate(context_results)]
            )

            # 3. Build system prompt
            if not system_prompt:
                system_prompt = (
                    "You are DevSkyy, an AI assistant with access to a knowledge base. "
                    "Answer questions accurately based on the provided context. "
                    "If the context doesn't contain relevant information, say so clearly. "
                    "Always cite sources when using information from the context."
                )

            # 4. Generate answer with Claude
            if not self.anthropic:
                # Fallback: return context only
                return {
                    "answer": "LLM not configured. Here are the relevant excerpts:\n\n" + context_str,
                    "sources": context_results,
                    "context_used": len(context_results),
                }

            message = self.anthropic.messages.create(
                model=model,
                max_tokens=RAGConfig.MAX_TOKENS,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": f"Context:\n{context_str}\n\nQuestion: {question}",
                    }
                ],
            )

            answer = message.content[0].text

            return {
                "answer": answer,
                "sources": context_results,
                "context_used": len(context_results),
                "model": model,
                "tokens_used": {
                    "input": message.usage.input_tokens,
                    "output": message.usage.output_tokens,
                },
            }

        except Exception as e:
            logger.error(f"Error processing RAG query: {e}")
            raise

    async def iterative_query(
        self,
        question: str,
        max_iterations: int = 3,
        min_results: int = 3,
        sufficiency_threshold: float = 0.75,
        model: str = RAGConfig.DEFAULT_MODEL,
        system_prompt: str | None = None,
    ) -> dict[str, Any]:
        """
        Iterative RAG query with multi-hop retrieval.

        Implements iterative retrieval loop:
        1. Initial retrieval based on question
        2. Evaluate if results are sufficient
        3. If not, reformulate query and retrieve again
        4. Repeat until sufficient context or max iterations
        5. Generate final answer with all context

        Perfect for:
        - Complex multi-hop questions
        - Questions requiring synthesis from multiple sources
        - Exploratory information gathering

        Args:
            question: User question (may require multiple retrievals)
            max_iterations: Maximum retrieval iterations (default 3)
            min_results: Minimum results needed before answering
            sufficiency_threshold: Similarity threshold for "sufficient" results
            model: LLM model to use
            system_prompt: Custom system prompt

        Returns:
            Answer with sources, iteration trace, and metadata
        """
        try:
            logger.info(f"ðŸ”„ Starting iterative RAG query: {question[:50]}...")

            all_results = []
            queries_used = [question]
            current_query = question
            iteration_trace = []

            for iteration in range(max_iterations):
                # Retrieve with current query
                results = await self.search(
                    query=current_query,
                    top_k=RAGConfig.TOP_K_RESULTS,
                    min_similarity=0.5  # Lower threshold for iteration
                )

                # Track iteration
                iteration_info = {
                    "iteration": iteration + 1,
                    "query": current_query,
                    "results_found": len(results),
                    "avg_similarity": sum(r["similarity"] for r in results) / max(len(results), 1)
                }
                iteration_trace.append(iteration_info)

                # Add unique results
                for result in results:
                    content = result.get("content", "")
                    if not any(r.get("content") == content for r in all_results):
                        result["iteration"] = iteration + 1
                        result["query"] = current_query
                        all_results.append(result)

                logger.info(
                    f"  Iteration {iteration + 1}: {len(results)} results, "
                    f"total unique: {len(all_results)}"
                )

                # Check sufficiency
                if len(all_results) >= min_results:
                    avg_similarity = sum(r["similarity"] for r in all_results) / len(all_results)
                    if avg_similarity >= sufficiency_threshold:
                        logger.info(f"  Sufficient results found (avg similarity: {avg_similarity:.2f})")
                        break

                # Reformulate query for next iteration
                if iteration < max_iterations - 1:
                    current_query = await self._reformulate_query(
                        original_question=question,
                        current_results=all_results,
                        iteration=iteration
                    )
                    queries_used.append(current_query)

            # Generate answer with all collected context
            if not all_results:
                return {
                    "answer": "I couldn't find any relevant information after multiple search attempts.",
                    "sources": [],
                    "context_used": 0,
                    "iterations": len(iteration_trace),
                    "queries_used": queries_used,
                    "iteration_trace": iteration_trace,
                }

            # Build combined context from all iterations
            context_str = "\n\n".join([
                f"[Source {idx + 1} (iter {r.get('iteration', 1)})] {r['content']}"
                for idx, r in enumerate(all_results[:10])  # Limit to top 10
            ])

            # Build enhanced system prompt for multi-source synthesis
            if not system_prompt:
                system_prompt = (
                    "You are DevSkyy, an AI assistant with access to a knowledge base. "
                    "You have retrieved information from multiple search queries to answer this question. "
                    "Synthesize information from all sources to provide a comprehensive answer. "
                    "Cite sources by their numbers when using specific information. "
                    "If sources conflict, note the discrepancy."
                )

            # Generate answer
            if not self.anthropic:
                return {
                    "answer": "LLM not configured. Here are the relevant excerpts:\n\n" + context_str,
                    "sources": all_results,
                    "context_used": len(all_results),
                    "iterations": len(iteration_trace),
                    "queries_used": queries_used,
                    "iteration_trace": iteration_trace,
                }

            message = self.anthropic.messages.create(
                model=model,
                max_tokens=RAGConfig.MAX_TOKENS,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": f"Context from {len(all_results)} sources:\n{context_str}\n\nQuestion: {question}",
                    }
                ],
            )

            answer = message.content[0].text

            logger.info(f"âœ… Iterative RAG complete: {len(iteration_trace)} iterations, {len(all_results)} sources")

            return {
                "answer": answer,
                "sources": all_results,
                "context_used": len(all_results),
                "iterations": len(iteration_trace),
                "queries_used": queries_used,
                "iteration_trace": iteration_trace,
                "model": model,
                "tokens_used": {
                    "input": message.usage.input_tokens,
                    "output": message.usage.output_tokens,
                },
            }

        except Exception as e:
            logger.error(f"Error in iterative RAG query: {e}")
            raise

    async def _reformulate_query(
        self,
        original_question: str,
        current_results: list[dict],
        iteration: int
    ) -> str:
        """
        Reformulate query based on current results to find missing information.

        Strategies by iteration:
        1. Add specificity (e.g., "detailed", "explanation")
        2. Try related terms/synonyms
        3. Break into sub-questions
        4. Focus on gaps in current results
        """
        # If we have an LLM, use it for smart reformulation
        if self.anthropic and iteration > 0:
            try:
                # Summarize what we found
                found_summary = "\n".join([
                    f"- {r['content'][:100]}..." for r in current_results[:3]
                ])

                prompt = f"""Given the original question and what we've found so far, suggest a reformulated search query to find additional relevant information.

Original question: {original_question}

Information found so far:
{found_summary}

Suggest a single search query that would help find information we're still missing. Return ONLY the query, nothing else."""

                response = self.anthropic.messages.create(
                    model="claude-sonnet-4-5-20250929",
                    max_tokens=100,
                    messages=[{"role": "user", "content": prompt}]
                )

                reformulated = response.content[0].text.strip()
                logger.debug(f"Reformulated query: {reformulated}")
                return reformulated

            except Exception as e:
                logger.warning(f"LLM reformulation failed, using fallback: {e}")

        # Fallback: rule-based reformulation
        reformulation_templates = [
            f"{original_question} detailed explanation",
            f"how does {original_question} work",
            f"{original_question} examples",
            f"{original_question} best practices",
            f"what is {original_question}",
        ]

        return reformulation_templates[iteration % len(reformulation_templates)]

    def get_stats(self) -> dict[str, Any]:
        """Get RAG system statistics"""
        return {
            "vector_db": self.vector_db.get_stats(),
            "config": {
                "chunk_size": self.doc_processor.chunk_size,
                "chunk_overlap": self.doc_processor.chunk_overlap,
                "top_k": RAGConfig.TOP_K_RESULTS,
                "similarity_threshold": RAGConfig.SIMILARITY_THRESHOLD,
            },
        }


# =============================================================================
# SINGLETON INSTANCE
# =============================================================================

# Global RAG service instance
_rag_service: RAGService | None = None


def get_rag_service() -> RAGService:
    """Get or create global RAG service instance"""
    global _rag_service

    if _rag_service is None:
        _rag_service = RAGService()
        logger.info("Initialized RAG service")

    return _rag_service


# =============================================================================
# USAGE EXAMPLE
# =============================================================================

if __name__ == "__main__":

    async def main():
        """Example usage"""
        rag = get_rag_service()

        # Ingest sample text
        sample_text = """
        DevSkyy is a multi-agent AI platform with 54 specialized agents.
        It provides enterprise-grade AI orchestration with RBAC, encryption,
        and comprehensive monitoring. The platform follows the Truth Protocol
        for verifiable, secure, and production-ready AI systems.
        """

        stats = await rag.ingest_text(sample_text, source="example")
        print(f"Ingested: {stats}")

        # Search
        results = await rag.search("What is DevSkyy?")
        print(f"\nSearch results: {len(results)}")
        for result in results:
            print(f"  - {result['content'][:100]}... (similarity: {result['similarity']:.2f})")

        # RAG query
        answer = await rag.query("What security features does DevSkyy have?")
        print(f"\nAnswer: {answer['answer']}")
        print(f"Sources used: {answer['context_used']}")

    asyncio.run(main())
