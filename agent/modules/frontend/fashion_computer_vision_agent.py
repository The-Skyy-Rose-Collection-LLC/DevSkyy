from datetime import datetime
from pathlib import Path
import os

from sklearn.cluster import KMeans
from PIL import Image
from diffusers import (
    StableDiffusionXLPipeline,
    StableVideoDiffusionPipeline,
    AnimateDiffPipeline,
    MotionAdapter,
    EulerDiscreteScheduler
)
from openai import AsyncOpenAI
from transformers import (
    CLIPModel,
    CLIPProcessor,
    ViTImageProcessor,
    ViTModel,
    Blip2Processor,
    Blip2ForConditionalGeneration
)
from typing import Any, Dict, List, Optional, Tuple, Union
import anthropic
import base64
import cv2
import io
import logging
import numpy as np
import torch
import moviepy.editor as mp
from moviepy.video.io.VideoFileClip import VideoFileClip
import tempfile
import asyncio
from concurrent.futures import ThreadPoolExecutor

"""
Fashion Computer Vision Agent
Advanced AI vision system specialized in luxury fashion analysis and generation

Features:
- High-quality fashion image generation (Stable Diffusion SDXL)
- Advanced video generation (Stable Video Diffusion + AnimateDiff)
- Fashion runway video creation (3-5 second luxury videos at 1024x576+)
- Product 360Â° rotation animations (24-step smooth rotations)
- Video upscaling to maximum quality (up to 2048x1152)
- Detailed fabric and texture analysis
- Stitch pattern recognition
- Garment cut and construction analysis
- Material identification (silk, cotton, leather, etc.)
- Quality assessment
- Style classification
- Color palette extraction
- Trend prediction from visual data
- Product photography optimization
- Virtual try-on preparation
- Defect detection for quality control
- Custom brand model training with LoRA fine-tuning
- Automatic image preprocessing and captioning
"""

logger = logging.getLogger(__name__)

class FashionComputerVisionAgent:
    """
    Advanced computer vision agent specialized in luxury fashion
    imagery analysis, generation, and quality assessment.
    """

    def __init__(self):
        # AI Services
        self.claude = anthropic.AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.openai = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # Initialize models
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ðŸ–¥ï¸ Using device: {self.device}")

        # Thread pool for CPU-intensive operations
        self.executor = ThreadPoolExecutor(max_workers=4)

        # Model loading flags to manage GPU memory
        self._models_loaded = {
            "clip": False,
            "vit": False,
            "sdxl": False,
            "svd": False,
            "animatediff": False,
            "blip2": False
        }

        # Storage paths
        self.video_storage = Path("generated_videos")
        self.video_storage.mkdir(exist_ok=True)
        self.model_storage = Path("custom_models")
        self.model_storage.mkdir(exist_ok=True)

        # Load core models
        self._load_core_models()

        logger.info("ðŸŽ¨ Fashion Computer Vision Agent initialized with video generation capabilities")

    def _load_core_models(self):
        """Load core models for fashion analysis and generation."""
        # Load CLIP for fashion understanding
        try:
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            self.clip_model.to(self.device)
            self._models_loaded["clip"] = True
            logger.info("âœ… CLIP model loaded for fashion analysis")
        except Exception as e:
            logger.warning(f"âš ï¸ CLIP model not loaded: {e}")
            self.clip_model = None

        # Load ViT for detailed image features
        try:
            self.vit_processor = ViTImageProcessor.from_pretrained("google/vit-large-patch16-224")
            self.vit_model = ViTModel.from_pretrained("google/vit-large-patch16-224")
            self.vit_model.to(self.device)
            self._models_loaded["vit"] = True
            logger.info("âœ… ViT model loaded for detailed analysis")
        except Exception as e:
            logger.warning(f"âš ï¸ ViT model not loaded: {e}")
            self.vit_model = None

        # Load BLIP-2 for image captioning
        try:
            self.blip2_processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
            self.blip2_model = Blip2ForConditionalGeneration.from_pretrained(
                "Salesforce/blip2-opt-2.7b",
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            self.blip2_model.to(self.device)
            self._models_loaded["blip2"] = True
            logger.info("âœ… BLIP-2 model loaded for image captioning")
        except Exception as e:
            logger.warning(f"âš ï¸ BLIP-2 model not loaded: {e}")
            self.blip2_model = None

    async def _load_image_generation_model(self):
        """Load SDXL model for image generation."""
        if self._models_loaded["sdxl"]:
            return

        try:
            self.sdxl_pipeline = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                use_safetensors=True,
            )
            self.sdxl_pipeline.to(self.device)
            self._models_loaded["sdxl"] = True
            logger.info("âœ… Stable Diffusion XL loaded for image generation")
        except Exception as e:
            logger.warning(f"âš ï¸ SDXL not loaded: {e}")
            self.sdxl_pipeline = None

    async def _load_video_generation_models(self):
        """Load video generation models (SVD and AnimateDiff)."""
        # Load Stable Video Diffusion
        if not self._models_loaded["svd"]:
            try:
                self.svd_pipeline = StableVideoDiffusionPipeline.from_pretrained(
                    "stabilityai/stable-video-diffusion-img2vid-xt",
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    variant="fp16" if self.device == "cuda" else None
                )
                self.svd_pipeline.to(self.device)
                self._models_loaded["svd"] = True
                logger.info("âœ… Stable Video Diffusion loaded for video generation")
            except Exception as e:
                logger.warning(f"âš ï¸ SVD not loaded: {e}")
                self.svd_pipeline = None

        # Load AnimateDiff
        if not self._models_loaded["animatediff"]:
            try:
                adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
                self.animatediff_pipeline = AnimateDiffPipeline.from_pretrained(
                    "runwayml/stable-diffusion-v1-5",
                    motion_adapter=adapter,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
                )
                self.animatediff_pipeline.scheduler = EulerDiscreteScheduler.from_config(
                    self.animatediff_pipeline.scheduler.config
                )
                self.animatediff_pipeline.to(self.device)
                self._models_loaded["animatediff"] = True
                logger.info("âœ… AnimateDiff loaded for animation generation")
            except Exception as e:
                logger.warning(f"âš ï¸ AnimateDiff not loaded: {e}")
                self.animatediff_pipeline = None

    def _unload_model(self, model_name: str):
        """Unload a specific model to free GPU memory."""
        if model_name == "sdxl" and hasattr(self, 'sdxl_pipeline') and self.sdxl_pipeline:
            del self.sdxl_pipeline
            self.sdxl_pipeline = None
            self._models_loaded["sdxl"] = False
        elif model_name == "svd" and hasattr(self, 'svd_pipeline') and self.svd_pipeline:
            del self.svd_pipeline
            self.svd_pipeline = None
            self._models_loaded["svd"] = False
        elif model_name == "animatediff" and hasattr(self, 'animatediff_pipeline') and self.animatediff_pipeline:
            del self.animatediff_pipeline
            self.animatediff_pipeline = None
            self._models_loaded["animatediff"] = False

        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info(f"ðŸ—‘ï¸ {model_name} model unloaded to free memory")

        # Fashion-specific knowledge base
        self.fabric_types = {
            "silk": {
                "characteristics": ["smooth", "lustrous", "drapes_well", "delicate"],
                "visual_cues": ["sheen", "soft_reflection", "flowing"],
            },
            "cotton": {
                "characteristics": ["matte", "breathable", "structured", "durable"],
                "visual_cues": ["texture_visible", "no_sheen", "crisp"],
            },
            "wool": {
                "characteristics": ["warm", "textured", "structured", "natural"],
                "visual_cues": ["visible_fibers", "matte", "thick"],
            },
            "leather": {
                "characteristics": [
                    "durable",
                    "structured",
                    "luxury",
                    "aged_beautifully",
                ],
                "visual_cues": ["grain_pattern", "natural_variations", "sheen"],
            },
            "linen": {
                "characteristics": [
                    "breathable",
                    "natural",
                    "wrinkles_easily",
                    "casual_luxury",
                ],
                "visual_cues": ["visible_weave", "natural_color", "texture"],
            },
            "cashmere": {
                "characteristics": ["ultra_soft", "luxury", "warm", "delicate"],
                "visual_cues": ["fine_texture", "soft_appearance", "subtle_sheen"],
            },
            "satin": {
                "characteristics": ["smooth", "glossy", "formal", "drapes_beautifully"],
                "visual_cues": ["high_sheen", "smooth_surface", "reflective"],
            },
            "velvet": {
                "characteristics": ["plush", "luxury", "textured", "rich"],
                "visual_cues": ["depth", "directional_pile", "rich_color"],
            },
        }

        self.garment_cuts = {
            "a_line": "Fitted at top, gradually widens toward hem",
            "empire": "High waist just below bust, flows down",
            "sheath": "Fitted silhouette following body shape",
            "shift": "Straight, loose fit not following curves",
            "wrap": "Overlaps and ties at waist",
            "ball_gown": "Fitted bodice with full skirt",
            "mermaid": "Fitted through body, flares at knees",
            "column": "Straight, narrow silhouette",
            "fit_and_flare": "Fitted bodice, flared skirt",
        }

        self.stitch_types = {
            "straight": "Basic linear stitch",
            "zigzag": "Side-to-side stitch for stretch fabrics",
            "overlock": "Edge finishing stitch",
            "blind_hem": "Invisible hemming",
            "topstitch": "Decorative visible stitching",
            "chain_stitch": "Looped interlocking stitches",
            "cross_stitch": "X-shaped decorative stitch",
        }

        logger.info("ðŸŽ¨ Fashion Computer Vision Agent initialized")

    async def analyze_fashion_image(
        self, image_path: Union[str, Path, Image.Image]
    ) -> Dict[str, Any]:
        """
        Comprehensive fashion image analysis including:
        - Fabric identification
        - Texture analysis
        - Stitching detection
        - Cut/silhouette analysis
        - Quality assessment
        - Style classification
        - Color analysis
        """
        try:
            logger.info("ðŸ” Analyzing fashion image...")

            # Load image
            if isinstance(image_path, (str, Path)):
                image = Image.open(image_path).convert("RGB")
            else:
                image = image_path

            # Multi-level analysis
            results = {}

            # 1. Fabric and texture analysis
            results["fabric_analysis"] = await self._analyze_fabric(image)

            # 2. Stitching analysis
            results["stitching_analysis"] = await self._analyze_stitching(image)

            # 3. Garment cut analysis
            results["cut_analysis"] = await self._analyze_garment_cut(image)

            # 4. Quality assessment
            results["quality_assessment"] = await self._assess_quality(image)

            # 5. Style classification
            results["style_classification"] = await self._classify_style(image)

            # 6. Color palette extraction
            results["color_palette"] = await self._extract_color_palette(image)

            # 7. AI-powered detailed analysis
            results["ai_detailed_analysis"] = await self._ai_vision_analysis(image)

            # 8. Overall assessment
            results["overall_assessment"] = self._generate_overall_assessment(results)

            results["timestamp"] = datetime.now().isoformat()
            results["analysis_complete"] = True

            logger.info("âœ… Fashion image analysis complete")
            return results

        except Exception as e:
            logger.error(f"âŒ Fashion image analysis failed: {e}")
            return {"error": str(e), "status": "failed"}

    async def _analyze_fabric(self, image: Image.Image) -> Dict[str, Any]:
        """
        Detailed fabric and texture analysis using computer vision.
        """
        try:
            # Convert to numpy for OpenCV processing
            img_array = np.array(image)
            img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

            # Texture analysis using Gabor filters
            texture_features = self._extract_texture_features(img_gray)

            # Sheen/glossiness detection
            sheen_level = self._detect_sheen(img_array)

            # Weave pattern detection
            weave_pattern = self._detect_weave_pattern(img_gray)

            # Identify fabric type using features
            fabric_predictions = self._predict_fabric_type(
                texture_features, sheen_level, weave_pattern
            )

            return {
                "primary_fabric": (
                    fabric_predictions[0] if fabric_predictions else "unknown"
                ),
                "fabric_confidence": fabric_predictions[1] if fabric_predictions else 0,
                "alternative_fabrics": (
                    fabric_predictions[2:] if len(fabric_predictions) > 2 else []
                ),
                "texture_score": texture_features["complexity"],
                "sheen_level": sheen_level,
                "weave_visible": weave_pattern["visible"],
                "fabric_characteristics": self._get_fabric_characteristics(
                    fabric_predictions[0] if fabric_predictions else "unknown"
                ),
            }

        except Exception as e:
            logger.error(f"âŒ Fabric analysis failed: {e}")
            return {"error": str(e)}

    def _extract_texture_features(self, gray_image: np.ndarray) -> Dict[str, Any]:
        """
        Extract texture features using Gabor filters and statistical analysis.
        """
        # Gabor filter for texture
        gabor_kernels = []
        for theta in np.arange(0, np.pi, np.pi / 4):
            kernel = cv2.getGaborKernel(
                (21, 21), 5.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F
            )
            gabor_kernels.append(kernel)

        features = []
        for kernel in gabor_kernels:
            filtered = cv2.filter2D(gray_image, cv2.CV_8UC3, kernel)
            features.append(filtered.mean())
            features.append(filtered.var())

        # Texture complexity
        laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)
        complexity = laplacian.var()

        return {"gabor_features": features, "complexity": float(complexity)}

    def _detect_sheen(self, image_array: np.ndarray) -> float:
        """
        Detect fabric sheen/glossiness level.
        """
        # Convert to HSV
        hsv = cv2.cvtColor(image_array, cv2.COLOR_RGB2HSV)

        # High value channel indicates sheen
        value_channel = hsv[:, :, 2]

        # Detect bright spots (sheen indicators)
        high_value_pixels = np.sum(value_channel > 200)
        total_pixels = value_channel.size

        sheen_level = high_value_pixels / total_pixels

        return float(sheen_level)

    def _detect_weave_pattern(self, gray_image: np.ndarray) -> Dict[str, Any]:
        """
        Detect visible weave patterns in fabric.
        """
        # FFT to detect periodic patterns
        f = np.fft.fft2(gray_image)
        fshift = np.fft.fftshift(f)
        magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1)

        # Check for periodic patterns
        pattern_strength = np.std(magnitude_spectrum)

        return {
            "visible": pattern_strength > 50,
            "pattern_strength": float(pattern_strength),
            "pattern_type": "regular_weave" if pattern_strength > 50 else "smooth",
        }

    def _predict_fabric_type(
        self, texture_features: Dict, sheen_level: float, weave_pattern: Dict
    ) -> List[Tuple[str, float]]:
        """
        Predict fabric type based on visual features.
        """
        predictions = []

        complexity = texture_features["complexity"]

        # Rule-based fabric prediction
        if sheen_level > 0.3:
            if complexity < 100:
                predictions.append(("satin", 0.85))
                predictions.append(("silk", 0.75))
            else:
                predictions.append(("silk", 0.80))
        elif sheen_level > 0.15:
            predictions.append(("leather", 0.70))
            predictions.append(("velvet", 0.60))
        else:
            if weave_pattern["visible"]:
                predictions.append(("linen", 0.80))
                predictions.append(("cotton", 0.70))
            elif complexity > 150:
                predictions.append(("wool", 0.75))
                predictions.append(("cashmere", 0.65))
            else:
                predictions.append(("cotton", 0.70))

        return predictions

    def _get_fabric_characteristics(self, fabric_type: str) -> Dict[str, Any]:
        """
        Get characteristics of identified fabric.
        """
        return self.fabric_types.get(
            fabric_type, {"characteristics": [], "visual_cues": []}
        )

    async def _analyze_stitching(self, image: Image.Image) -> Dict[str, Any]:
        """
        Analyze stitching patterns and quality.
        """
        try:
            img_array = np.array(image)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)

            # Edge detection for stitch lines
            edges = cv2.Canny(gray, 50, 150)

            # Hough line detection for straight stitches
            lines = cv2.HoughLinesP(
                edges, 1, np.pi / 180, threshold=100, minLineLength=30, maxLineGap=10
            )

            stitch_count = len(lines) if lines is not None else 0

            # Analyze stitch uniformity
            uniformity = (
                self._analyze_stitch_uniformity(lines) if lines is not None else 0
            )

            # Detect stitch type
            stitch_types = self._detect_stitch_types(edges, lines)

            return {
                "stitches_detected": stitch_count,
                "stitch_uniformity": uniformity,
                "stitch_quality": (
                    "excellent"
                    if uniformity > 0.8
                    else "good" if uniformity > 0.6 else "fair"
                ),
                "detected_stitch_types": stitch_types,
                "visible_stitching": stitch_count > 10,
            }

        except Exception as e:
            logger.error(f"âŒ Stitching analysis failed: {e}")
            return {"error": str(e)}

    def _analyze_stitch_uniformity(self, lines: np.ndarray) -> float:
        """
        Analyze uniformity of stitching.
        """
        if lines is None or len(lines) < 2:
            return 0.0

        # Calculate line lengths
        lengths = []
        for line in lines:
            x1, y1, x2, y2 = line[0]
            length = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)
            lengths.append(length)

        # Uniformity = 1 - (std / mean)
        if len(lengths) > 0 and np.mean(lengths) > 0:
            uniformity = 1 - (np.std(lengths) / np.mean(lengths))
            return max(0, min(1, uniformity))

        return 0.0

    def _detect_stitch_types(
        self, edges: np.ndarray, lines: Optional[np.ndarray]
    ) -> List[str]:
        """
        Detect types of stitches present.
        """
        stitch_types = []

        if lines is not None and len(lines) > 5:
            stitch_types.append("straight")

            # Check for topstitching (visible decorative)
            if len(lines) > 20:
                stitch_types.append("topstitch")

        # Detect zigzag patterns
        contours, _ = cv2.findContours(
            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )
        for contour in contours:
            if len(contour) > 10:
                # Approximate contour
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)
                if len(approx) > 6:  # Zigzag has many vertices
                    if "zigzag" not in stitch_types:
                        stitch_types.append("zigzag")

        return stitch_types if stitch_types else ["standard"]

    async def _analyze_garment_cut(self, image: Image.Image) -> Dict[str, Any]:
        """
        Analyze garment cut and silhouette.
        """
        try:
            # Use AI vision for cut analysis
            img_array = np.array(image)

            # Detect silhouette using contour detection
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

            contours, _ = cv2.findContours(
                binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
            )

            if contours:
                # Get largest contour (garment outline)
                largest_contour = max(contours, key=cv2.contourArea)

                # Analyze shape
                silhouette_type = self._classify_silhouette(largest_contour)

                # Calculate shape metrics
                area = cv2.contourArea(largest_contour)
                perimeter = cv2.arcLength(largest_contour, True)
                hull = cv2.convexHull(largest_contour)
                hull_area = cv2.contourArea(hull)

                # Solidity (how fitted vs flowing)
                solidity = area / hull_area if hull_area > 0 else 0

                return {
                    "cut_type": silhouette_type,
                    "cut_description": self.garment_cuts.get(
                        silhouette_type, "Unknown cut"
                    ),
                    "fit_type": (
                        "fitted"
                        if solidity > 0.8
                        else "flowing" if solidity < 0.5 else "semi_fitted"
                    ),
                    "silhouette_complexity": len(largest_contour),
                    "shape_metrics": {
                        "area": float(area),
                        "perimeter": float(perimeter),
                        "solidity": float(solidity),
                    },
                }

            return {
                "cut_type": "unknown",
                "message": "Could not detect garment outline",
            }

        except Exception as e:
            logger.error(f"âŒ Cut analysis failed: {e}")
            return {"error": str(e)}

    def _classify_silhouette(self, contour: np.ndarray) -> str:
        """
        Classify garment silhouette from contour.
        """
        # Approximate contour
        epsilon = 0.01 * cv2.arcLength(contour, True)
        cv2.approxPolyDP(contour, epsilon, True)

        # Get bounding rectangle
        x, y, w, h = cv2.boundingRect(contour)

        # Aspect ratio
        aspect_ratio = h / w if w > 0 else 0

        # Simple classification based on shape
        if aspect_ratio > 1.5:
            # Tall and narrow
            moments = cv2.moments(contour)
            if moments["m00"] != 0:
                # Check if wider at bottom (A-line) or top
                return "column" if w < h * 0.4 else "a_line"
        elif aspect_ratio < 0.8:
            # Wide and short
            return "shift"
        else:
            # Check fitting
            hull = cv2.convexHull(contour)
            hull_area = cv2.contourArea(hull)
            area = cv2.contourArea(contour)
            solidity = area / hull_area if hull_area > 0 else 0

            if solidity > 0.85:
                return "sheath"
            else:
                return "fit_and_flare"

        return "unknown"

    async def _assess_quality(self, image: Image.Image) -> Dict[str, Any]:
        """
        Assess garment quality from image.
        """
        try:
            img_array = np.array(image)

            # Sharpness (indicates quality photography and detail)
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()

            # Color consistency
            color_variance = np.var(img_array, axis=(0, 1))
            color_consistency = 1 / (1 + np.mean(color_variance) / 1000)

            # Overall quality score
            quality_score = (sharpness / 1000 + color_consistency) / 2

            quality_level = (
                "excellent"
                if quality_score > 0.8
                else (
                    "good"
                    if quality_score > 0.6
                    else "fair" if quality_score > 0.4 else "poor"
                )
            )

            return {
                "quality_score": float(min(1.0, quality_score)),
                "quality_level": quality_level,
                "sharpness": float(sharpness),
                "color_consistency": float(color_consistency),
                "assessment": f"This garment shows {quality_level} quality based on visual analysis",
            }

        except Exception as e:
            logger.error(f"âŒ Quality assessment failed: {e}")
            return {"error": str(e)}

    async def _classify_style(self, image: Image.Image) -> Dict[str, Any]:
        """
        Classify fashion style using CLIP model.
        """
        try:
            if not self.clip_model:
                return {"style": "unknown", "reason": "CLIP model not loaded"}

            # Fashion style categories
            style_categories = [
                "casual wear",
                "formal wear",
                "evening gown",
                "business attire",
                "cocktail dress",
                "luxury fashion",
                "streetwear",
                "haute couture",
                "resort wear",
                "bridal wear",
            ]

            # Process image and text
            inputs = self.clip_processor(
                text=style_categories, images=image, return_tensors="pt", padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Get predictions
            with torch.no_grad():
                outputs = self.clip_model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)

            # Get top 3 predictions
            top_probs, top_indices = torch.topk(probs[0], 3)

            predictions = [
                {"style": style_categories[idx.item()], "confidence": prob.item()}
                for prob, idx in zip(top_probs, top_indices)
            ]

            return {
                "primary_style": predictions[0]["style"],
                "confidence": predictions[0]["confidence"],
                "alternative_styles": predictions[1:],
            }

        except Exception as e:
            logger.error(f"âŒ Style classification failed: {e}")
            return {"error": str(e)}

    async def _extract_color_palette(self, image: Image.Image) -> Dict[str, Any]:
        """
        Extract dominant color palette from garment.
        """
        try:
            # Resize for faster processing
            img_small = image.resize((150, 150))
            img_array = np.array(img_small)

            # Reshape to list of pixels
            pixels = img_array.reshape(-1, 3)

            # Use k-means clustering to find dominant colors

            n_colors = 5
            kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
            kmeans.fit(pixels)

            # Get colors and their percentages
            colors = kmeans.cluster_centers_.astype(int)
            labels = kmeans.labels_
            counts = np.bincount(labels)
            percentages = counts / len(labels)

            # Sort by percentage
            sorted_indices = np.argsort(percentages)[::-1]

            palette = []
            for idx in sorted_indices:
                color_rgb = colors[idx]
                palette.append(
                    {
                        "rgb": color_rgb.tolist(),
                        "hex": "#{:02x}{:02x}{:02x}".format(*color_rgb),
                        "percentage": float(percentages[idx]),
                    }
                )

            return {"palette": palette, "dominant_color": palette[0]}

        except Exception as e:
            logger.error(f"âŒ Color extraction failed: {e}")
            return {"error": str(e)}

    async def _ai_vision_analysis(self, image: Image.Image) -> Dict[str, Any]:
        """
        Use Claude's vision capabilities for detailed analysis.
        """
        try:
            # Convert image to base64
            buffered = io.BytesIO()
            image.save(buffered, format="PNG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()

            # Analyze with Claude
            response = await self.claude.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=2000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/png",
                                    "data": img_base64,
                                },
                            },
                            {
                                "type": "text",
                                "text": """Analyze this luxury fashion garment in detail:

1. Fabric type and quality
2. Texture description
3. Stitching and construction details
4. Garment cut and silhouette
5. Style classification
6. Quality indicators
7. Unique design elements
8. Craftsmanship level
9. Estimated luxury tier (accessible, premium, haute couture)
10. Recommended styling and occasions

Provide detailed, expert fashion analysis.""",
                            },
                        ],
                    }
                ],
            )

            analysis = response.content[0].text

            return {"detailed_analysis": analysis, "model": "claude-sonnet-4.5"}

        except Exception as e:
            logger.error(f"âŒ AI vision analysis failed: {e}")
            return {"error": str(e)}

    def _generate_overall_assessment(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate comprehensive overall assessment.
        """
        fabric = results.get("fabric_analysis", {})
        quality = results.get("quality_assessment", {})
        style = results.get("style_classification", {})

        assessment = {
            "luxury_tier": self._determine_luxury_tier(fabric, quality),
            "craftsmanship_rating": quality.get("quality_level", "unknown"),
            "authenticity_confidence": (
                "high" if quality.get("quality_score", 0) > 0.7 else "medium"
            ),
            "estimated_value_range": self._estimate_value_range(fabric, quality, style),
            "recommended_use": style.get("primary_style", "versatile wear"),
        }

        return assessment

    def _determine_luxury_tier(self, fabric: Dict, quality: Dict) -> str:
        """
        Determine luxury tier of garment.
        """
        fabric_type = fabric.get("primary_fabric", "").lower()
        quality_score = quality.get("quality_score", 0)

        luxury_fabrics = ["silk", "cashmere", "satin", "velvet", "leather"]

        if fabric_type in luxury_fabrics and quality_score > 0.8:
            return "haute_couture"
        elif fabric_type in luxury_fabrics or quality_score > 0.7:
            return "premium_luxury"
        elif quality_score > 0.5:
            return "accessible_luxury"
        else:
            return "standard"

    def _estimate_value_range(self, fabric: Dict, quality: Dict, style: Dict) -> str:
        """
        Estimate value range based on analysis.
        """
        tier = self._determine_luxury_tier(fabric, quality)

        value_ranges = {
            "haute_couture": "$5,000 - $50,000+",
            "premium_luxury": "$1,000 - $5,000",
            "accessible_luxury": "$300 - $1,000",
            "standard": "$50 - $300",
        }

        return value_ranges.get(tier, "Unknown")

    async def generate_fashion_image(
        self,
        prompt: str,
        style: str = "luxury fashion photography",
        negative_prompt: Optional[str] = None,
        width: int = 1024,
        height: int = 1024,
    ) -> Dict[str, Any]:
        """
        Generate high-quality fashion imagery using Stable Diffusion XL.
        """
        try:
            if not self.sdxl_pipeline:
                return {
                    "error": "Image generation not available - SDXL not loaded",
                    "status": "failed",
                }

            logger.info(f"ðŸŽ¨ Generating fashion image: {prompt}")

            # Enhance prompt for luxury fashion
            enhanced_prompt = f"{prompt}, {style}, high quality, professional photography, studio lighting, sharp focus, detailed texture, 8k uhd, luxury aesthetic"

            # Default negative prompt for fashion
            if not negative_prompt:
                negative_prompt = "low quality, blurry, distorted, amateur, bad anatomy, watermark, text, logo"

            # Generate image
            image = self.sdxl_pipeline(
                prompt=enhanced_prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=50,
                guidance_scale=7.5,
            ).images[0]

            # Save image
            output_path = (
                Path("generated_fashion")
                / f"fashion_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            )
            output_path.parent.mkdir(exist_ok=True)
            image.save(output_path)

            return {
                "success": True,
                "image_path": str(output_path),
                "prompt_used": enhanced_prompt,
                "dimensions": {"width": width, "height": height},
                "model": "stable-diffusion-xl",
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"âŒ Image generation failed: {e}")
            return {"error": str(e), "status": "failed"}

    async def generate_fashion_runway_video(
        self,
        prompt: str,
        duration: int = 4,
        fps: int = 8,
        width: int = 1024,
        height: int = 576,
        style: str = "luxury fashion runway",
        upscale: bool = True
    ) -> Dict[str, Any]:
        """
        Generate luxury fashion runway videos using Stable Video Diffusion.

        Args:
            prompt: Description of the fashion scene
            duration: Video duration in seconds (3-5 recommended)
            fps: Frames per second (8-24)
            width: Video width (minimum 1024)
            height: Video height (minimum 576)
            style: Video style description
            upscale: Whether to upscale to maximum quality

        Returns:
            Dict with video file path and metadata
        """
        try:
            logger.info(f"ðŸŽ¬ Generating fashion runway video: {prompt}")

            # Load video generation models
            await self._load_video_generation_models()

            if not hasattr(self, 'svd_pipeline') or not self.svd_pipeline:
                return {
                    "error": "Video generation not available - SVD not loaded",
                    "status": "failed"
                }

            # First generate a high-quality image as the starting frame
            await self._load_image_generation_model()

            enhanced_prompt = f"{prompt}, {style}, professional runway photography, high fashion, cinematic lighting, 8k uhd, luxury aesthetic, model walking"
            negative_prompt = "low quality, blurry, distorted, amateur, bad anatomy, watermark, text, logo, static, still"

            # Generate initial image
            if self.sdxl_pipeline:
                initial_image = self.sdxl_pipeline(
                    prompt=enhanced_prompt,
                    negative_prompt=negative_prompt,
                    width=width,
                    height=height,
                    num_inference_steps=50,
                    guidance_scale=7.5,
                ).images[0]
            else:
                return {"error": "Image generation required for video creation", "status": "failed"}

            # Generate video from image
            num_frames = duration * fps
            video_frames = self.svd_pipeline(
                initial_image,
                decode_chunk_size=8,
                num_frames=num_frames,
                motion_bucket_id=127,  # Higher motion
                fps=fps,
                noise_aug_strength=0.1
            ).frames[0]

            # Save video
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"runway_video_{timestamp}.mp4"
            output_path = self.video_storage / filename

            # Convert frames to video using moviepy
            await self._frames_to_video(
                video_frames,
                output_path,
                fps,
                upscale_target=(2048, 1152) if upscale else None
            )

            # Unload video model to free memory
            self._unload_model("svd")

            return {
                "success": True,
                "video_path": str(output_path),
                "prompt_used": enhanced_prompt,
                "duration": duration,
                "fps": fps,
                "dimensions": {"width": width, "height": height},
                "model": "stable-video-diffusion",
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"âŒ Runway video generation failed: {e}")
            return {"error": str(e), "status": "failed"}

    async def generate_product_360_video(
        self,
        product_image_path: Union[str, Path],
        rotation_steps: int = 24,
        duration: int = 3,
        upscale: bool = True
    ) -> Dict[str, Any]:
        """
        Generate 360Â° product rotation video using AnimateDiff.

        Args:
            product_image_path: Path to product image
            rotation_steps: Number of rotation steps (24 = 15Â° per step)
            duration: Video duration in seconds
            upscale: Whether to upscale to maximum quality

        Returns:
            Dict with video file path and metadata
        """
        try:
            logger.info(f"ðŸ”„ Generating 360Â° product video: {product_image_path}")

            # Load video generation models
            await self._load_video_generation_models()

            if not hasattr(self, 'animatediff_pipeline') or not self.animatediff_pipeline:
                return {
                    "error": "Product animation not available - AnimateDiff not loaded",
                    "status": "failed"
                }

            # Load and preprocess product image
            product_image = Image.open(product_image_path).convert("RGB")

            # Generate rotation prompt
            rotation_prompt = f"luxury fashion product rotating 360 degrees, smooth rotation, professional product photography, studio lighting, white background, high quality, detailed texture"
            negative_prompt = "blurry, distorted, multiple products, hands, people, low quality, amateur"

            # Generate rotation video
            video_frames = self.animatediff_pipeline(
                prompt=rotation_prompt,
                negative_prompt=negative_prompt,
                num_frames=rotation_steps,
                guidance_scale=7.5,
                num_inference_steps=25,
                generator=torch.Generator(device=self.device).manual_seed(42)
            ).frames[0]

            # Save video
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"product_360_{timestamp}.mp4"
            output_path = self.video_storage / filename

            fps = rotation_steps // duration

            # Convert frames to video
            await self._frames_to_video(
                video_frames,
                output_path,
                fps,
                upscale_target=(2048, 2048) if upscale else None
            )

            # Unload animation model to free memory
            self._unload_model("animatediff")

            return {
                "success": True,
                "video_path": str(output_path),
                "product_image": str(product_image_path),
                "rotation_steps": rotation_steps,
                "duration": duration,
                "fps": fps,
                "model": "animatediff",
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"âŒ Product 360Â° video generation failed: {e}")
            return {"error": str(e), "status": "failed"}

    async def _frames_to_video(
        self,
        frames: List[Image.Image],
        output_path: Path,
        fps: int,
        upscale_target: Optional[Tuple[int, int]] = None
    ):
        """
        Convert frames to MP4 video with H.264 encoding.

        Args:
            frames: List of PIL Images
            output_path: Output video file path
            fps: Frames per second
            upscale_target: Target resolution for upscaling (width, height)
        """
        def process_frames():
            # Convert PIL images to numpy arrays
            frame_arrays = []
            for frame in frames:
                if upscale_target:
                    # Upscale frame using high-quality resampling
                    frame = frame.resize(upscale_target, Image.Resampling.LANCZOS)

                frame_array = np.array(frame)
                frame_arrays.append(frame_array)

            # Create video clip
            clip = mp.ImageSequenceClip(frame_arrays, fps=fps)

            # Write video with H.264 encoding
            clip.write_videofile(
                str(output_path),
                codec='libx264',
                audio=False,
                verbose=False,
                logger=None,
                temp_audiofile_path=None,
                remove_temp=True
            )

            clip.close()

        # Run in thread pool to avoid blocking
        await asyncio.get_event_loop().run_in_executor(
            self.executor, process_frames
        )

        logger.info(f"âœ… Video saved: {output_path}")

    async def generate_image_caption(self, image: Union[str, Path, Image.Image]) -> str:
        """
        Generate automatic caption for image using BLIP-2.

        Args:
            image: Image path or PIL Image

        Returns:
            Generated caption string
        """
        try:
            if not hasattr(self, 'blip2_model') or not self.blip2_model:
                return "luxury fashion item"

            # Load image
            if isinstance(image, (str, Path)):
                image = Image.open(image).convert("RGB")

            # Generate caption
            inputs = self.blip2_processor(image, return_tensors="pt").to(self.device)

            with torch.no_grad():
                generated_ids = self.blip2_model.generate(**inputs, max_length=50)
                caption = self.blip2_processor.batch_decode(
                    generated_ids, skip_special_tokens=True
                )[0].strip()

            return caption

        except Exception as e:
            logger.warning(f"Caption generation failed: {e}")
            return "luxury fashion item"

    async def upscale_video(
        self,
        video_path: Union[str, Path],
        target_resolution: Tuple[int, int] = (2048, 1152)
    ) -> Dict[str, Any]:
        """
        Upscale video to higher resolution using advanced interpolation.

        Args:
            video_path: Path to input video
            target_resolution: Target (width, height)

        Returns:
            Dict with upscaled video path and metadata
        """
        try:
            logger.info(f"â¬†ï¸ Upscaling video to {target_resolution}")

            def upscale_process():
                # Load video
                clip = VideoFileClip(str(video_path))

                # Upscale using high-quality resampling
                upscaled_clip = clip.resize(target_resolution)

                # Save upscaled video
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                upscaled_path = self.video_storage / f"upscaled_{timestamp}.mp4"

                upscaled_clip.write_videofile(
                    str(upscaled_path),
                    codec='libx264',
                    audio=False,
                    verbose=False,
                    logger=None
                )

                clip.close()
                upscaled_clip.close()

                return upscaled_path

            # Run in thread pool
            upscaled_path = await asyncio.get_event_loop().run_in_executor(
                self.executor, upscale_process
            )

            return {
                "success": True,
                "upscaled_video_path": str(upscaled_path),
                "original_video_path": str(video_path),
                "target_resolution": target_resolution,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ Video upscaling failed: {e}")
            return {"error": str(e), "status": "failed"}

# Factory function
def create_fashion_vision_agent() -> FashionComputerVisionAgent:
    """Create Fashion Computer Vision Agent instance."""
    return FashionComputerVisionAgent()

# Global instance
fashion_vision_agent = create_fashion_vision_agent()

# Convenience functions
async def analyze_garment(image_path: Union[str, Path]) -> Dict[str, Any]:
    """Analyze fashion garment from image."""
    return await fashion_vision_agent.analyze_fashion_image(image_path)

async def generate_fashion_photo(prompt: str, style: str = "luxury") -> Dict[str, Any]:
    """Generate fashion photography."""
    return await fashion_vision_agent.generate_fashion_image(prompt, style)

async def generate_runway_video(prompt: str, duration: int = 4, fps: int = 8) -> Dict[str, Any]:
    """Generate luxury fashion runway video."""
    return await fashion_vision_agent.generate_fashion_runway_video(prompt, duration, fps)

async def generate_product_rotation(product_image_path: Union[str, Path], rotation_steps: int = 24) -> Dict[str, Any]:
    """Generate 360Â° product rotation video."""
    return await fashion_vision_agent.generate_product_360_video(product_image_path, rotation_steps)

async def caption_image(image_path: Union[str, Path]) -> str:
    """Generate automatic caption for fashion image."""
    return await fashion_vision_agent.generate_image_caption(image_path)
