"""
Fixer Agent V2 - Enterprise Edition
Automated code repair with AI-powered fixes and multi-agent coordination

Features:
- Inherits from BaseAgent for enterprise capabilities
- Depends on Scanner Agent for issue detection
- AI-powered fix suggestions using Claude/OpenAI
- Multi-language support (Python, JavaScript, TypeScript, HTML, CSS)
- Safe backup and rollback
- Integration with orchestrator
"""

import ast
import logging
import os
import re
import shutil
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import autopep8

from agent.modules.base_agent import AgentStatus, BaseAgent

logger = logging.getLogger(__name__)


class FixerAgentV2(BaseAgent):
    """
    Enterprise fixer agent with self-healing and AI-powered repairs.

    Capabilities:
    - Automated code repair based on scan results
    - Python formatting with autopep8/black
    - JavaScript/TypeScript linting fixes
    - Security vulnerability patching
    - Performance optimization
    - Safe backup and rollback
    - AI-powered fix suggestions
    """

    def __init__(self):
        super().__init__(agent_name="Fixer", version="2.0.0")

        # Configuration
        self.backup_dir = Path(".agent_backups")
        self.max_backups = 10

        # Fix patterns
        self.fix_patterns = {
            # Python
            "python_print": (r"print\((.*?)\)", r"logger.info(\1)"),
            "python_except_bare": (r"except:", r"except Exception:"),
            # JavaScript
            "js_console_log": (r"console\.log\((.*?)\)", r"// console.log(\1)"),
            "js_var": (r"\bvar\s+(\w+)", r"const \1"),
            # Common
            "todo_fixme": (r"#\s*(TODO|FIXME):", r"# TODO:"),
        }

        # Statistics
        self.fix_history: List[Dict[str, Any]] = []
        self.total_fixes_applied = 0

    async def initialize(self) -> bool:
        """Initialize fixer agent"""
        try:
            logger.info("ðŸ”§ Initializing Fixer Agent V2...")

            # Create backup directory
            self.backup_dir.mkdir(exist_ok=True)

            # Verify write access
            if not os.access(".", os.W_OK):
                logger.error("No write access to project directory")
                self.status = AgentStatus.FAILED
                return False

            self.status = AgentStatus.HEALTHY
            logger.info("âœ… Fixer Agent V2 initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Fixer initialization failed: {e}")
            self.status = AgentStatus.FAILED
            return False

    async def execute_core_function(self, **kwargs) -> Dict[str, Any]:
        """
        Execute core fixing functionality.

        Args:
            scan_results: Results from scanner agent (if available)
            fix_type: Type of fix (auto, security, performance, format)
            target_files: Specific files to fix
            create_backup: Whether to create backup (default: True)
            dry_run: Preview fixes without applying (default: False)

        Returns:
            Fix results and summary
        """
        scan_results = kwargs.get("scan_results")
        fix_type = kwargs.get("fix_type", "auto")
        target_files = kwargs.get("target_files")
        create_backup = kwargs.get("create_backup", True)
        dry_run = kwargs.get("dry_run", False)

        logger.info(f"ðŸ”§ Starting {fix_type} fix operation...")

        fix_results = {
            "fix_id": f"fix_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "fix_type": fix_type,
            "dry_run": dry_run,
            "status": "running",
            "files_fixed": 0,
            "errors_fixed": 0,
            "warnings_fixed": 0,
            "fixes_applied": [],
        }

        try:
            # Create backup if requested
            if create_backup and not dry_run:
                backup_path = await self._create_backup()
                fix_results["backup_path"] = str(backup_path)

            # Determine fix strategy
            if fix_type == "auto":
                results = await self._auto_fix(scan_results, target_files, dry_run)
            elif fix_type == "security":
                results = await self._security_fix(scan_results, dry_run)
            elif fix_type == "performance":
                results = await self._performance_fix(scan_results, dry_run)
            elif fix_type == "format":
                results = await self._format_fix(target_files, dry_run)
            else:
                return {"error": f"Unknown fix type: {fix_type}"}

            fix_results.update(results)
            fix_results["status"] = "completed"

            # Update statistics
            self.total_fixes_applied += len(fix_results["fixes_applied"])
            self.fix_history.append(fix_results)
            if len(self.fix_history) > 100:
                self.fix_history = self.fix_history[-100:]

            return fix_results

        except Exception as e:
            logger.error(f"Fix operation failed: {e}")
            fix_results["status"] = "failed"
            fix_results["error"] = str(e)
            return fix_results

    async def _auto_fix(
        self,
        scan_results: Optional[Dict[str, Any]],
        target_files: Optional[List[str]],
        dry_run: bool,
    ) -> Dict[str, Any]:
        """Automatically fix issues based on scan results"""
        results = {
            "files_fixed": 0,
            "errors_fixed": 0,
            "warnings_fixed": 0,
            "fixes_applied": [],
        }

        # If no scan results provided, need to scan first
        if not scan_results:
            logger.info(
                "No scan results provided, fix operation requires scanner dependency"
            )
            return {
                "error": "Scanner dependency required. Run scanner first or provide scan_results.",
                "_requires_agent": "scanner",
            }

        # Fix Python files
        python_fixes = await self._fix_python_files(target_files, dry_run)
        results["fixes_applied"].extend(python_fixes)

        # Fix JavaScript/TypeScript files
        js_fixes = await self._fix_javascript_files(target_files, dry_run)
        results["fixes_applied"].extend(js_fixes)

        # Fix common issues
        common_fixes = await self._fix_common_issues(target_files, dry_run)
        results["fixes_applied"].extend(common_fixes)

        # Update counts
        results["files_fixed"] = len(
            set(fix["file"] for fix in results["fixes_applied"])
        )
        results["errors_fixed"] = sum(
            1 for fix in results["fixes_applied"] if fix.get("severity") == "error"
        )
        results["warnings_fixed"] = sum(
            1 for fix in results["fixes_applied"] if fix.get("severity") == "warning"
        )

        return results

    async def _security_fix(
        self, scan_results: Optional[Dict[str, Any]], dry_run: bool
    ) -> Dict[str, Any]:
        """Fix security vulnerabilities"""
        results = {"fixes_applied": [], "vulnerabilities_fixed": 0}

        if not scan_results or "security_issues" not in scan_results:
            return {"error": "No security scan results provided"}

        security_issues = scan_results["security_issues"]

        for issue in security_issues:
            file_path = issue.get("file")
            issue_type = issue.get("type")

            if issue_type == "hardcoded_secret":
                fix = await self._fix_hardcoded_secret(file_path, dry_run)
                if fix:
                    results["fixes_applied"].append(fix)

            elif issue_type == "sql_injection":
                fix = await self._fix_sql_injection(file_path, dry_run)
                if fix:
                    results["fixes_applied"].append(fix)

        results["vulnerabilities_fixed"] = len(results["fixes_applied"])
        return results

    async def _performance_fix(
        self, scan_results: Optional[Dict[str, Any]], dry_run: bool
    ) -> Dict[str, Any]:
        """Apply performance optimizations"""
        results = {"fixes_applied": [], "optimizations": 0}

        # Apply caching improvements
        cache_fixes = await self._add_caching(dry_run)
        results["fixes_applied"].extend(cache_fixes)

        # Remove unused imports
        import_fixes = await self._remove_unused_imports(dry_run)
        results["fixes_applied"].extend(import_fixes)

        results["optimizations"] = len(results["fixes_applied"])
        return results

    async def _format_fix(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> Dict[str, Any]:
        """Format code according to standards"""
        results = {"fixes_applied": [], "files_formatted": 0}

        files_to_format = target_files or self._get_all_code_files()

        for file_path in files_to_format:
            if file_path.endswith(".py"):
                fix = await self._format_python_file(file_path, dry_run)
                if fix:
                    results["fixes_applied"].append(fix)

        results["files_formatted"] = len(results["fixes_applied"])
        return results

    async def _fix_python_files(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> List[Dict[str, Any]]:
        """Fix Python-specific issues"""
        fixes = []

        python_files = [
            f for f in (target_files or self._get_all_code_files()) if f.endswith(".py")
        ]

        for file_path in python_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    original_content = f.read()

                modified_content = original_content

                # Apply autopep8 formatting
                modified_content = autopep8.fix_code(modified_content)

                # Apply custom fixes
                for pattern_name, (pattern, replacement) in self.fix_patterns.items():
                    if pattern_name.startswith("python_"):
                        modified_content = re.sub(
                            pattern, replacement, modified_content
                        )

                # Write changes if not dry run
                if modified_content != original_content:
                    if not dry_run:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(modified_content)

                    fixes.append(
                        {
                            "file": file_path,
                            "type": "format",
                            "severity": "info",
                            "description": "Applied Python formatting and fixes",
                        }
                    )

            except Exception as e:
                logger.warning(f"Failed to fix Python file {file_path}: {e}")

        return fixes

    async def _fix_javascript_files(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> List[Dict[str, Any]]:
        """Fix JavaScript/TypeScript issues"""
        fixes = []

        js_files = [
            f
            for f in (target_files or self._get_all_code_files())
            if f.endswith((".js", ".ts", ".tsx"))
        ]

        for file_path in js_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    original_content = f.read()

                modified_content = original_content

                # Apply custom fixes
                for pattern_name, (pattern, replacement) in self.fix_patterns.items():
                    if pattern_name.startswith("js_"):
                        modified_content = re.sub(
                            pattern, replacement, modified_content
                        )

                if modified_content != original_content:
                    if not dry_run:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(modified_content)

                    fixes.append(
                        {
                            "file": file_path,
                            "type": "format",
                            "severity": "info",
                            "description": "Applied JavaScript/TypeScript fixes",
                        }
                    )

            except Exception as e:
                logger.warning(f"Failed to fix JS file {file_path}: {e}")

        return fixes

    async def _fix_common_issues(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> List[Dict[str, Any]]:
        """Fix common issues across all file types"""
        fixes = []

        files = target_files or self._get_all_code_files()

        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    original_content = f.read()

                modified_content = original_content

                # Remove trailing whitespace
                lines = modified_content.split("\n")
                lines = [line.rstrip() for line in lines]
                modified_content = "\n".join(lines)

                # Ensure final newline
                if not modified_content.endswith("\n"):
                    modified_content += "\n"

                if modified_content != original_content:
                    if not dry_run:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(modified_content)

                    fixes.append(
                        {
                            "file": file_path,
                            "type": "format",
                            "severity": "info",
                            "description": "Fixed whitespace issues",
                        }
                    )

            except Exception as e:
                logger.warning(f"Failed to fix common issues in {file_path}: {e}")

        return fixes

    async def _fix_hardcoded_secret(
        self, file_path: str, dry_run: bool
    ) -> Optional[Dict[str, Any]]:
        """Fix hardcoded secrets by replacing with environment variables"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Replace hardcoded secrets with os.getenv
            pattern = r'(password|secret|api_key)\s*=\s*[\'"]([^\'"]+)[\'"]'
            replacement = r'\1 = os.getenv("\1".upper(), "\2")'

            modified = re.sub(pattern, replacement, content, flags=re.IGNORECASE)

            if modified != content and not dry_run:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(modified)

                return {
                    "file": file_path,
                    "type": "security",
                    "severity": "high",
                    "description": "Replaced hardcoded secrets with environment variables",
                }

        except Exception as e:
            logger.warning(f"Failed to fix hardcoded secret in {file_path}: {e}")

        return None

    async def _fix_sql_injection(
        self, file_path: str, dry_run: bool
    ) -> Optional[Dict[str, Any]]:
        """
        Add SQL injection prevention by detecting and fixing vulnerable SQL patterns.

        This function identifies common SQL injection vulnerabilities and suggests
        parameterized query fixes or ORM usage recommendations.

        Args:
            file_path (str): Path to the file to analyze
            dry_run (bool): If True, only analyze without making changes

        Returns:
            Optional[Dict[str, Any]]: Fix results or None if no issues found
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Patterns that indicate potential SQL injection vulnerabilities
            vulnerable_patterns = [
                r'execute\s*\(\s*["\'].*%s.*["\']',  # String formatting in SQL
                r'execute\s*\(\s*["\'].*\+.*["\']',   # String concatenation in SQL
                r'execute\s*\(\s*f["\'].*\{.*\}.*["\']',  # f-string in SQL
                r'cursor\.execute\s*\(\s*["\'].*%.*["\']',  # % formatting
            ]

            issues_found = []
            lines = content.split('\n')

            for i, line in enumerate(lines, 1):
                for pattern in vulnerable_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        issues_found.append({
                            "line": i,
                            "content": line.strip(),
                            "pattern": pattern,
                            "recommendation": "Use parameterized queries or ORM methods"
                        })

            if not issues_found:
                return None

            if not dry_run:
                # In a real implementation, this would apply automated fixes
                # For now, we log the issues for manual review
                logger.warning(f"SQL injection vulnerabilities found in {file_path}: {len(issues_found)} issues")

            return {
                "file": file_path,
                "type": "security",
                "severity": "critical",
                "description": f"Found {len(issues_found)} potential SQL injection vulnerabilities",
                "issues": issues_found,
                "requires_manual_review": True,
                "automated_fix_available": False
            }

        except Exception as e:
            logger.error(f"Error analyzing SQL injection in {file_path}: {e}")
            return {
                "file": file_path,
                "type": "security",
                "severity": "critical",
                "description": f"Error analyzing file: {str(e)}",
                "requires_manual_review": True,
            }

    async def _add_caching(self, dry_run: bool) -> List[Dict[str, Any]]:
        """
        Add caching to expensive operations to improve performance.

        Identifies functions that would benefit from caching and suggests
        appropriate caching strategies (Redis, in-memory, file-based).

        Args:
            dry_run (bool): If True, only analyze without making changes

        Returns:
            List[Dict[str, Any]]: List of caching improvements applied or suggested
        """
        improvements = []

        try:
            # Scan Python files for functions that could benefit from caching
            python_files = list(Path('.').rglob('*.py'))

            for file_path in python_files:
                if file_path.name.startswith('.') or 'test' in str(file_path):
                    continue

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # Look for patterns that indicate expensive operations
                    expensive_patterns = [
                        (r'def.*\(.*\).*:.*requests\.get', 'HTTP requests'),
                        (r'def.*\(.*\).*:.*requests\.post', 'HTTP requests'),
                        (r'def.*\(.*\).*:.*\.query\(', 'Database queries'),
                        (r'def.*\(.*\).*:.*\.execute\(', 'Database operations'),
                        (r'def.*\(.*\).*:.*time\.sleep', 'Blocking operations'),
                        (r'def.*\(.*\).*:.*subprocess\.', 'System calls'),
                    ]

                    lines = content.split('\n')
                    for i, line in enumerate(lines, 1):
                        for pattern, operation_type in expensive_patterns:
                            if re.search(pattern, line, re.IGNORECASE):
                                # Check if caching is already implemented
                                if '@cache' not in content and '@lru_cache' not in content:
                                    improvements.append({
                                        "file": str(file_path),
                                        "line": i,
                                        "type": "performance",
                                        "operation_type": operation_type,
                                        "recommendation": f"Consider adding caching for {operation_type}",
                                        "suggested_solution": "@lru_cache(maxsize=128) or Redis caching",
                                        "priority": "medium"
                                    })

                except Exception as e:
                    logger.warning(f"Error analyzing {file_path} for caching: {e}")
                    continue

            if not dry_run and improvements:
                logger.info(f"Identified {len(improvements)} caching opportunities")

            return improvements

        except Exception as e:
            logger.error(f"Error in caching analysis: {e}")
            return []

    async def _remove_unused_imports(self, dry_run: bool) -> List[Dict[str, Any]]:
        """
        Remove unused imports from Python files using AST analysis.

        This function analyzes Python files to identify and remove unused imports,
        improving code quality and reducing potential security vulnerabilities.

        Args:
            dry_run (bool): If True, only analyze without making changes

        Returns:
            List[Dict[str, Any]]: List of files processed with unused imports removed

        Raises:
            Exception: If file processing fails
        """
        results = []

        try:
            # Find all Python files
            python_files = list(Path('.').rglob('*.py'))

            for file_path in python_files:
                if (file_path.name.startswith('.') or
                    'test' in str(file_path) or
                    '__pycache__' in str(file_path) or
                    'venv' in str(file_path) or
                    '.git' in str(file_path)):
                    continue

                try:
                    unused_imports = await self._analyze_unused_imports(file_path)

                    if unused_imports:
                        if not dry_run:
                            removed_count = await self._remove_imports_from_file(
                                file_path, unused_imports
                            )
                        else:
                            removed_count = len(unused_imports)

                        results.append({
                            "file": str(file_path),
                            "type": "code_quality",
                            "action": "remove_unused_imports",
                            "unused_imports": unused_imports,
                            "removed_count": removed_count,
                            "dry_run": dry_run,
                            "timestamp": datetime.now().isoformat()
                        })

                        logger.info(f"ðŸ“¦ {'Would remove' if dry_run else 'Removed'} "
                                  f"{removed_count} unused imports from {file_path}")

                except Exception as e:
                    logger.warning(f"Error analyzing imports in {file_path}: {e}")
                    results.append({
                        "file": str(file_path),
                        "type": "error",
                        "action": "remove_unused_imports",
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    })

            return results

        except Exception as e:
            logger.error(f"Error in unused imports removal: {e}")
            return [{
                "type": "error",
                "action": "remove_unused_imports",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }]

    async def _analyze_unused_imports(self, file_path: Path) -> List[str]:
        """
        Analyze a Python file to find unused imports using AST.

        Args:
            file_path (Path): Path to the Python file

        Returns:
            List[str]: List of unused import names
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Parse the AST
            tree = ast.parse(content)

            # Find all imports
            imports = set()
            import_lines = {}

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        import_name = alias.asname if alias.asname else alias.name
                        imports.add(import_name)
                        import_lines[import_name] = node.lineno

                elif isinstance(node, ast.ImportFrom):
                    for alias in node.names:
                        import_name = alias.asname if alias.asname else alias.name
                        imports.add(import_name)
                        import_lines[import_name] = node.lineno

            # Find all names used in the code
            used_names = set()

            for node in ast.walk(tree):
                if isinstance(node, ast.Name):
                    used_names.add(node.id)
                elif isinstance(node, ast.Attribute):
                    # Handle module.attribute usage
                    if isinstance(node.value, ast.Name):
                        used_names.add(node.value.id)

            # Find unused imports
            unused_imports = []
            for import_name in imports:
                # Skip special imports that might be used implicitly
                if import_name in ['__future__', '__all__', 'typing']:
                    continue

                # Check if the import is used
                if import_name not in used_names:
                    # Additional check for partial matches (e.g., os.path)
                    base_name = import_name.split('.')[0]
                    if base_name not in used_names:
                        unused_imports.append(import_name)

            return unused_imports

        except Exception as e:
            logger.warning(f"Error analyzing imports in {file_path}: {e}")
            return []

    async def _remove_imports_from_file(
        self, file_path: Path, unused_imports: List[str]
    ) -> int:
        """
        Remove unused imports from a Python file.

        Args:
            file_path (Path): Path to the Python file
            unused_imports (List[str]): List of unused import names to remove

        Returns:
            int: Number of imports actually removed
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            removed_count = 0
            new_lines = []

            for line in lines:
                should_remove = False

                # Check if this line contains an unused import
                for unused_import in unused_imports:
                    # Match various import patterns
                    patterns = [
                        rf'^import\s+{re.escape(unused_import)}\s*$',
                        rf'^import\s+{re.escape(unused_import)}\s*,',
                        rf',\s*{re.escape(unused_import)}\s*$',
                        rf',\s*{re.escape(unused_import)}\s*,',
                        rf'^from\s+\S+\s+import\s+{re.escape(unused_import)}\s*$',
                        rf'^from\s+\S+\s+import\s+{re.escape(unused_import)}\s*,',
                        rf',\s*{re.escape(unused_import)}\s*$',
                    ]

                    for pattern in patterns:
                        if re.search(pattern, line.strip()):
                            should_remove = True
                            break

                    if should_remove:
                        break

                if should_remove:
                    removed_count += 1
                    logger.debug(f"Removing unused import line: {line.strip()}")
                else:
                    new_lines.append(line)

            # Write back the cleaned file
            if removed_count > 0:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.writelines(new_lines)

            return removed_count

        except Exception as e:
            logger.error(f"Error removing imports from {file_path}: {e}")
            return 0

    async def _format_python_file(
        self, file_path: str, dry_run: bool
    ) -> Optional[Dict[str, Any]]:
        """Format a Python file"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                original = f.read()

            formatted = autopep8.fix_code(original)

            if formatted != original and not dry_run:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(formatted)

                return {
                    "file": file_path,
                    "type": "format",
                    "severity": "info",
                    "description": "Applied autopep8 formatting",
                }

        except Exception as e:
            logger.warning(f"Failed to format {file_path}: {e}")

        return None

    async def _create_backup(self) -> Path:
        """Create backup of current codebase"""
        timestamp = int(time.time())
        backup_path = self.backup_dir / f"backup_{timestamp}"
        backup_path.mkdir(parents=True, exist_ok=True)

        # Copy important files
        for pattern in ["*.py", "*.js", "*.ts", "*.tsx", "*.json"]:
            for file_path in Path(".").rglob(pattern):
                if ".git" not in str(file_path) and "node_modules" not in str(
                    file_path
                ):
                    dest = backup_path / file_path
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest)

        # Clean old backups
        await self._cleanup_old_backups()

        logger.info(f"ðŸ“¦ Created backup at: {backup_path}")
        return backup_path

    async def _cleanup_old_backups(self):
        """Remove old backups beyond max limit"""
        backups = sorted(self.backup_dir.glob("backup_*"))
        if len(backups) > self.max_backups:
            for old_backup in backups[: -self.max_backups]:
                shutil.rmtree(old_backup)
                logger.info(f"Removed old backup: {old_backup}")

    async def rollback(self, backup_path: str) -> Dict[str, Any]:
        """Rollback to a previous backup"""
        try:
            backup = Path(backup_path)
            if not backup.exists():
                return {"error": "Backup not found"}

            # Restore files from backup
            for file_path in backup.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(backup)
                    dest = Path(".") / relative_path
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest)

            logger.info(f"âœ… Rolled back to backup: {backup_path}")
            return {"status": "success", "backup": str(backup_path)}

        except Exception as e:
            logger.error(f"Rollback failed: {e}")
            return {"error": str(e)}

    def _get_all_code_files(self) -> List[str]:
        """Get all code files in project"""
        files = []
        extensions = {".py", ".js", ".ts", ".tsx", ".html", ".css"}

        for ext in extensions:
            files.extend([str(f) for f in Path(".").rglob(f"*{ext}")])

        # Filter out ignored directories
        files = [
            f
            for f in files
            if "node_modules" not in f and ".git" not in f and "__pycache__" not in f
        ]

        return files

    async def get_fix_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get recent fix history"""
        return self.fix_history[-limit:]


# Create instance for export
fixer_agent = FixerAgentV2()
