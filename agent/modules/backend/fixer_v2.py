from datetime import datetime
from pathlib import Path
import os
import re
import sys
import time

from agent.modules.base_agent import AgentStatus, BaseAgent
from typing import Any, Dict, List, Optional, Set, Tuple, Union
import ast
import autopep8
import logging
import shutil
import subprocess

"""
Fixer Agent V2 - Enterprise Edition
Automated code repair with AI-powered fixes and multi-agent coordination

Features:
    - Inherits from BaseAgent for enterprise capabilities
- Depends on Scanner Agent for issue detection
- AI-powered fix suggestions using Claude/OpenAI
- Multi-language support (Python, JavaScript, TypeScript, HTML, CSS)
- Safe backup and rollback
- Integration with orchestrator
"""

logger = logging.getLogger(__name__)

class FixerAgentV2(BaseAgent):
    """
    Enterprise fixer agent with self-healing and AI-powered repairs.

    Capabilities:
    - Automated code repair based on scan results
    - Python formatting with autopep8/black
    - JavaScript/TypeScript linting fixes
    - Security vulnerability patching
    - Performance optimization
    - Safe backup and rollback
    - AI-powered fix suggestions
    """

    def __init__(self):
        super().__init__(agent_name="Fixer", version="2.0.0")

        # Configuration
        self.backup_dir = Path(".agent_backups")
        self.max_backups = 10

        # Fix patterns
        self.fix_patterns = {
            # Python
            "python_print": (r"print\((.*?)\)", r"logger.info(\1)"),
            "python_except_bare": (r"except Exception:", r"except Exception:"),
            # JavaScript
            "js_console_log": (r"console\.log\((.*?)\)", r"// console.log(\1)"),
            "js_var": (r"\bvar\s+(\w+)", r"const \1"),
            # Common
            "todo_fixme": (r"#\s*(TODO|FIXME):", r"# TODO:"),
        }

        # Statistics
        self.fix_history: List[Dict[str, Any]] = []
        self.total_fixes_applied = 0

    async def initialize(self) -> bool:
        """Initialize fixer agent"""
        try:
            logger.info("ðŸ”§ Initializing Fixer Agent V2...")

            # Create backup directory
            self.backup_dir.mkdir(exist_ok=True)

            # Verify write access
            if not os.access(".", os.W_OK):
                logger.error("No write access to project directory")
                self.status = AgentStatus.FAILED
                return False

            self.status = AgentStatus.HEALTHY
            logger.info("âœ… Fixer Agent V2 initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Fixer initialization failed: {e}")
            self.status = AgentStatus.FAILED
            return False

    async def execute_core_function(self, **kwargs) -> Dict[str, Any]:
        """
        Execute core fixing functionality.

        Args:
            scan_results: Results from scanner agent (if available)
            fix_type: Type of fix (auto, security, performance, format)
            target_files: Specific files to fix
            create_backup: Whether to create backup (default: True)
            dry_run: Preview fixes without applying (default: False)

        Returns:
            Fix results and summary
        """
        scan_results = kwargs.get("scan_results")
        fix_type = kwargs.get("fix_type", "auto")
        target_files = kwargs.get("target_files")
        create_backup = kwargs.get("create_backup", True)
        dry_run = kwargs.get("dry_run", False)

        logger.info(f"ðŸ”§ Starting {fix_type} fix operation...")

        fix_results = {
            "fix_id": f"fix_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "fix_type": fix_type,
            "dry_run": dry_run,
            "status": "running",
            "files_fixed": 0,
            "errors_fixed": 0,
            "warnings_fixed": 0,
            "fixes_applied": [],
        }

        try:
            # Create backup if requested
            if create_backup and not dry_run:
                backup_path = await self._create_backup()
                fix_results["backup_path"] = str(backup_path)

            # Determine fix strategy
            if fix_type == "auto":
                results = await self._auto_fix(scan_results, target_files, dry_run)
            elif fix_type == "security":
                results = await self._security_fix(scan_results, dry_run)
            elif fix_type == "performance":
                results = await self._performance_fix(scan_results, dry_run)
            elif fix_type == "format":
                results = await self._format_fix(target_files, dry_run)
            else:
                return {"error": f"Unknown fix type: {fix_type}"}

            fix_results.update(results)
            fix_results["status"] = "completed"

            # Update statistics
            self.total_fixes_applied += len(fix_results["fixes_applied"])
            self.fix_history.append(fix_results)
            if len(self.fix_history) > 100:
                self.fix_history = self.fix_history[-100:]

            return fix_results

        except Exception as e:
            logger.error(f"Fix operation failed: {e}")
            fix_results["status"] = "failed"
            fix_results["error"] = str(e)
            return fix_results

    async def _auto_fix(
        self,
        scan_results: Optional[Dict[str, Any]],
        target_files: Optional[List[str]],
        dry_run: bool,
    ) -> Dict[str, Any]:
        """Automatically fix issues based on scan results"""
        results = {
            "files_fixed": 0,
            "errors_fixed": 0,
            "warnings_fixed": 0,
            "fixes_applied": [],
        }

        # If no scan results provided, need to scan first
        if not scan_results:
            logger.info(
                "No scan results provided, fix operation requires scanner dependency"
            )
            return {
                "error": "Scanner dependency required. Run scanner first or provide scan_results.",
                "_requires_agent": "scanner",
            }

        # Fix Python files
        python_fixes = await self._fix_python_files(target_files, dry_run)
        results["fixes_applied"].extend(python_fixes)

        # Fix JavaScript/TypeScript files
        js_fixes = await self._fix_javascript_files(target_files, dry_run)
        results["fixes_applied"].extend(js_fixes)

        # Fix common issues
        common_fixes = await self._fix_common_issues(target_files, dry_run)
        results["fixes_applied"].extend(common_fixes)

        # Update counts
        results["files_fixed"] = len()
            set(fix["file"] for fix in results["fixes_applied"])
        )
        results["errors_fixed"] = sum()
            1 for fix in results["fixes_applied"] if fix.get("severity") == "error"
        )
        results["warnings_fixed"] = sum()
            1 for fix in results["fixes_applied"] if fix.get("severity") == "warning"
        )

        return results

    async def _security_fix(
        self, scan_results: Optional[Dict[str, Any]], dry_run: bool
    ) -> Dict[str, Any]:
        """Fix security vulnerabilities"""
        results = {"fixes_applied": [], "vulnerabilities_fixed": 0}

        if not scan_results or "security_issues" not in scan_results:
            return {"error": "No security scan results provided"}

        security_issues = scan_results["security_issues"]

        for issue in security_issues:
            file_path = issue.get("file")
            issue_type = issue.get("type")

            if issue_type == "hardcoded_secret":
                fix = await self._fix_hardcoded_secret(file_path, dry_run)
                if fix:
                    results["fixes_applied"].append(fix)

            elif issue_type == "sql_injection":
                fix = await self._fix_sql_injection(file_path, dry_run)
                if fix:
                    results["fixes_applied"].append(fix)

        results["vulnerabilities_fixed"] = len(results["fixes_applied"])
        return results

    async def _performance_fix(
        self, scan_results: Optional[Dict[str, Any]], dry_run: bool
    ) -> Dict[str, Any]:
        """Apply performance optimizations"""
        results = {"fixes_applied": [], "optimizations": 0}

        # Apply caching improvements
        cache_fixes = await self._add_caching(dry_run)
        results["fixes_applied"].extend(cache_fixes)

        # Remove unused imports
        import_fixes = await self._remove_unused_imports(dry_run)
        results["fixes_applied"].extend(import_fixes)

        results["optimizations"] = len(results["fixes_applied"])
        return results

    async def _format_fix(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> Dict[str, Any]:
        """Format code according to standards"""
        results = {"fixes_applied": [], "files_formatted": 0}

        files_to_format = target_files or self._get_all_code_files()

        for file_path in files_to_format:
            if file_path.endswith(".py"):
                fix = await self._format_python_file(file_path, dry_run)
                if fix:
                    results["fixes_applied"].append(fix)

        results["files_formatted"] = len(results["fixes_applied"])
        return results

    async def _fix_python_files(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> List[Dict[str, Any]]:
        """Fix Python-specific issues"""
        fixes = []

        python_files = [
            f for f in (target_files or self._get_all_code_files()) if f.endswith(".py")
        ]

        for file_path in python_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    original_content = f.read()

                modified_content = original_content

                # Apply autopep8 formatting
                modified_content = autopep8.fix_code(modified_content)

                # Apply custom fixes
                for pattern_name, (pattern, replacement) in self.fix_patterns.items():
                    if pattern_name.startswith("python_"):
                        modified_content = re.sub(
                            pattern, replacement, modified_content
                        )

                # Write changes if not dry run
                if modified_content != original_content:
                    if not dry_run:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(modified_content)

                    fixes.append(
                        {
                            "file": file_path,
                            "type": "format",
                            "severity": "info",
                            "description": "Applied Python formatting and fixes",
                        }
                    )

            except Exception as e:
                logger.warning(f"Failed to fix Python file {file_path}: {e}")

        return fixes

    async def _fix_javascript_files(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> List[Dict[str, Any]]:
        """Fix JavaScript/TypeScript issues"""
        fixes = []

        js_files = [
            f
            for f in (target_files or self._get_all_code_files())
            if f.endswith((".js", ".ts", ".tsx"))
        ]

        for file_path in js_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    original_content = f.read()

                modified_content = original_content

                # Apply custom fixes
                for pattern_name, (pattern, replacement) in self.fix_patterns.items():
                    if pattern_name.startswith("js_"):
                        modified_content = re.sub(
                            pattern, replacement, modified_content
                        )

                if modified_content != original_content:
                    if not dry_run:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(modified_content)

                    fixes.append(
                        {
                            "file": file_path,
                            "type": "format",
                            "severity": "info",
                            "description": "Applied JavaScript/TypeScript fixes",
                        }
                    )

            except Exception as e:
                logger.warning(f"Failed to fix JS file {file_path}: {e}")

        return fixes

    async def _fix_common_issues(
        self, target_files: Optional[List[str]], dry_run: bool
    ) -> List[Dict[str, Any]]:
        """Fix common issues across all file types"""
        fixes = []

        files = target_files or self._get_all_code_files()

        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    original_content = f.read()

                modified_content = original_content

                # Remove trailing whitespace
                lines = modified_content.split("\n")
                lines = [line.rstrip() for line in lines]
                modified_content = "\n".join(lines)

                # Ensure final newline
                if not modified_content.endswith("\n"):
                    # Use list and join for string building
modified_content_list.append(...)

                if modified_content != original_content:
                    if not dry_run:
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(modified_content)

                    fixes.append(
                        {
                            "file": file_path,
                            "type": "format",
                            "severity": "info",
                            "description": "Fixed whitespace issues",
                        }
                    )

            except Exception as e:
                logger.warning(f"Failed to fix common issues in {file_path}: {e}")

        return fixes

    async def _fix_hardcoded_secret(
        self, file_path: str, dry_run: bool
    ) -> Optional[Dict[str, Any]]:
        """Fix hardcoded secrets by replacing with environment variables"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Replace hardcoded secrets with os.getenv
            pattern = r'(password|secret|api_key)\s*=\s*[\'"]([^\'"]+)[\'"]'
            replacement = r'\1 = os.getenv("\1".upper(), "\2")'

            modified = re.sub(pattern, replacement, content, flags=re.IGNORECASE)

            if modified != content and not dry_run:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(modified)

                return {
                    "file": file_path,
                    "type": "security",
                    "severity": "high",
                    "description": "Replaced hardcoded secrets with environment variables",
                }

        except Exception as e:
            logger.warning(f"Failed to fix hardcoded secret in {file_path}: {e}")

        return None

    async def _fix_sql_injection(
        self, file_path: str, dry_run: bool
    ) -> Optional[Dict[str, Any]]:
        """
        Add SQL injection prevention by detecting and fixing vulnerable SQL patterns.

        This function identifies common SQL injection vulnerabilities and suggests
        parameterized query fixes or ORM usage recommendations.

        Args:
            file_path (str): Path to the file to analyze
            dry_run (bool): If True, only analyze without making changes

        Returns:
            Optional[Dict[str, Any]]: Fix results or None if no issues found
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Patterns that indicate potential SQL injection vulnerabilities
            vulnerable_patterns = [
                r'execute\s*\(\s*["\'].*%s.*["\']',  # String formatting in SQL
                r'execute\s*\(\s*["\'].*\+.*["\']',   # String concatenation in SQL
                r'execute\s*\(\s*f["\'].*\{.*\}.*["\']',  # f-string in SQL
                r'cursor\.execute\s*\(\s*["\'].*%.*["\']',  # % formatting
            ]

            issues_found = []
            lines = content.split('\n')

            for i, line in enumerate(lines, 1):
                for pattern in vulnerable_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        issues_found.append({
                            "line": i,
                            "content": line.strip(),
                            "pattern": pattern,
                            "recommendation": "Use parameterized queries or ORM methods"
                        })

            if not issues_found:
                return None

            if not dry_run:
                # In a real implementation, this would apply automated fixes
                # For now, we log the issues for manual review
                logger.warning(f"SQL injection vulnerabilities found in {file_path}: {len(issues_found)} issues")

            return {
                "file": file_path,
                "type": "security",
                "severity": "critical",
                "description": f"Found {len(issues_found)} potential SQL injection vulnerabilities",
                "issues": issues_found,
                "requires_manual_review": True,
                "automated_fix_available": False
            }

        except Exception as e:
            logger.error(f"Error analyzing SQL injection in {file_path}: {e}")
            return {
                "file": file_path,
                "type": "security",
                "severity": "critical",
                "description": f"Error analyzing file: {str(e)}",
                "requires_manual_review": True,
            }

    async def _add_caching(self, dry_run: bool) -> List[Dict[str, Any]]:
        """
        Add caching to expensive operations to improve performance.

        Identifies functions that would benefit from caching and suggests
        appropriate caching strategies (Redis, in-memory, file-based).

        Args:
            dry_run (bool): If True, only analyze without making changes

        Returns:
            List[Dict[str, Any]]: List of caching improvements applied or suggested
        """
        improvements = []

        try:
            # Scan Python files for functions that could benefit from caching
            python_files = list(Path('.').rglob('*.py'))

            for file_path in python_files:
                if file_path.name.startswith('.') or 'test' in str(file_path):
                    continue

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # Look for patterns that indicate expensive operations
                    expensive_patterns = [
                        (r'def.*\(.*\).*:.*requests\.get', 'HTTP requests'),
                        (r'def.*\(.*\).*:.*requests\.post', 'HTTP requests'),
                        (r'def.*\(.*\).*:.*\.query\(', 'Database queries'),
                        (r'def.*\(.*\).*:.*\.execute\(', 'Database operations'),
                        (r'def.*\(.*\).*:.*time\.sleep', 'Blocking operations'),
                        (r'def.*\(.*\).*:.*subprocess\.', 'System calls'),
                    ]

                    lines = content.split('\n')
                    for i, line in enumerate(lines, 1):
                        for pattern, operation_type in expensive_patterns:
                            if re.search(pattern, line, re.IGNORECASE):
                                # Check if caching is already implemented
                                if '@cache' not in content and '@lru_cache' not in content:
                                    improvements.append({
                                        "file": str(file_path),
                                        "line": i,
                                        "type": "performance",
                                        "operation_type": operation_type,
                                        "recommendation": f"Consider adding caching for {operation_type}",
                                        "suggested_solution": "@lru_cache(maxsize=128) or Redis caching",
                                        "priority": "medium"
                                    })

                except Exception as e:
                    logger.warning(f"Error analyzing {file_path} for caching: {e}")
                    continue

            if not dry_run and improvements:
                logger.info(f"Identified {len(improvements)} caching opportunities")

            return improvements

        except Exception as e:
            logger.error(f"Error in caching analysis: {e}")
            return []

    async def _remove_unused_imports(self, dry_run: bool) -> List[Dict[str, Any]]:
        """
        Remove unused imports from Python files using AST analysis.

        This function analyzes Python files to identify and remove unused imports,
        improving code quality and reducing potential security vulnerabilities.

        Args:
            dry_run (bool): If True, only analyze without making changes

        Returns:
            List[Dict[str, Any]]: List of files processed with unused imports removed

        Raises:
            Exception: If file processing fails
        """
        results = []

        try:
            # Find all Python files
            python_files = list(Path('.').rglob('*.py'))

            for file_path in python_files:
                if (file_path.name.startswith('.') or
                    'test' in str(file_path) or
                    '__pycache__' in str(file_path) or
                    'venv' in str(file_path) or
                    '.git' in str(file_path)):
                    continue

                try:
                    unused_imports = await self._analyze_unused_imports(file_path)

                    if unused_imports:
                        if not dry_run:
                            removed_count = await self._remove_imports_from_file(
                                file_path, unused_imports
                            )
                        else:
                            removed_count = len(unused_imports)

                        results.append({
                            "file": str(file_path),
                            "type": "code_quality",
                            "action": "remove_unused_imports",
                            "unused_imports": unused_imports,
                            "removed_count": removed_count,
                            "dry_run": dry_run,
                            "timestamp": datetime.now().isoformat()
                        })

                        logger.info(f"ðŸ“¦ {'Would remove' if dry_run else 'Removed'} "
                                  f"{removed_count} unused imports from {file_path}")

                except Exception as e:
                    logger.warning(f"Error analyzing imports in {file_path}: {e}")
                    results.append({
                        "file": str(file_path),
                        "type": "error",
                        "action": "remove_unused_imports",
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    })

            return results

        except Exception as e:
            logger.error(f"Error in unused imports removal: {e}")
            return [{
                "type": "error",
                "action": "remove_unused_imports",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }]

    async def _analyze_unused_imports(self, file_path: Path) -> List[str]:
        """
        Analyze a Python file to find unused imports using AST.

        Args:
            file_path (Path): Path to the Python file

        Returns:
            List[str]: List of unused import names
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Parse the AST
            tree = ast.parse(content)

            # Find all imports
            imports = set()
            import_lines = {}

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        import_name = alias.asname if alias.asname else alias.name
                        imports.add(import_name)
                        import_lines[import_name] = node.lineno

                elif isinstance(node, ast.ImportFrom):
                    for alias in node.names:
                        import_name = alias.asname if alias.asname else alias.name
                        imports.add(import_name)
                        import_lines[import_name] = node.lineno

            # Find all names used in the code
            used_names = set()

            for node in ast.walk(tree):
                if isinstance(node, ast.Name):
                    used_names.add(node.id)
                elif isinstance(node, ast.Attribute):
                    # Handle module.attribute usage
                    if isinstance(node.value, ast.Name):
                        used_names.add(node.value.id)

            # Find unused imports
            unused_imports = []
            for import_name in imports:
                # Skip special imports that might be used implicitly
                if import_name in ['__future__', '__all__', 'typing']:
                    continue

                # Check if the import is used
                if import_name not in used_names:
                    # Additional check for partial matches (e.g., os.path)
                    base_name = import_name.split('.')[0]
                    if base_name not in used_names:
                        unused_imports.append(import_name)

            return unused_imports

        except Exception as e:
            logger.warning(f"Error analyzing imports in {file_path}: {e}")
            return []

    async def _remove_imports_from_file(
        self, file_path: Path, unused_imports: List[str]
    ) -> int:
        """
        Remove unused imports from a Python file.

        Args:
            file_path (Path): Path to the Python file
            unused_imports (List[str]): List of unused import names to remove

        Returns:
            int: Number of imports actually removed
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            removed_count = 0
            new_lines = []

            for line in lines:
                should_remove = False

                # Check if this line contains an unused import
                for unused_import in unused_imports:
                    # Match various import patterns
                    patterns = [
                        rf'^import\s+{re.escape(unused_import)}\s*$',
                        rf'^import\s+{re.escape(unused_import)}\s*,',
                        rf',\s*{re.escape(unused_import)}\s*$',
                        rf',\s*{re.escape(unused_import)}\s*,',
                        rf'^from\s+\S+\s+import\s+{re.escape(unused_import)}\s*$',
                        rf'^from\s+\S+\s+import\s+{re.escape(unused_import)}\s*,',
                        rf',\s*{re.escape(unused_import)}\s*$',
                    ]

                    for pattern in patterns:
                        if re.search(pattern, line.strip()):
                            should_remove = True
                            break

                    if should_remove:
                        break

                if should_remove:
                    removed_count += 1
                    logger.debug(f"Removing unused import line: {line.strip()}")
                else:
                    new_lines.append(line)

            # Write back the cleaned file
            if removed_count > 0:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.writelines(new_lines)

            return removed_count

        except Exception as e:
            logger.error(f"Error removing imports from {file_path}: {e}")
            return 0

    async def _format_python_file(
        self, file_path: str, dry_run: bool
    ) -> Optional[Dict[str, Any]]:
        """Format a Python file"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                original = f.read()

            formatted = autopep8.fix_code(original)

            if formatted != original and not dry_run:
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(formatted)

                return {
                    "file": file_path,
                    "type": "format",
                    "severity": "info",
                    "description": "Applied autopep8 formatting",
                }

        except Exception as e:
            logger.warning(f"Failed to format {file_path}: {e}")

        return None

    async def _create_backup(self) -> Path:
        """Create backup of current codebase"""
        timestamp = int(time.time())
        backup_path = self.backup_dir / f"backup_{timestamp}"
        backup_path.mkdir(parents=True, exist_ok=True)

        # Copy important files
        for pattern in ["*.py", "*.js", "*.ts", "*.tsx", "*.json"]:
            for file_path in Path(".").rglob(pattern):
                if ".git" not in str(file_path) and "node_modules" not in str(
                    file_path
                ):
                    dest = backup_path / file_path
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest)

        # Clean old backups
        await self._cleanup_old_backups()

        logger.info(f"ðŸ“¦ Created backup at: {backup_path}")
        return backup_path

    async def _cleanup_old_backups(self):
        """Remove old backups beyond max limit"""
        backups = sorted(self.backup_dir.glob("backup_*"))
        if len(backups) > self.max_backups:
            for old_backup in backups[: -self.max_backups]:
                shutil.rmtree(old_backup)
                logger.info(f"Removed old backup: {old_backup}")

    async def rollback(self, backup_path: str) -> Dict[str, Any]:
        """Rollback to a previous backup"""
        try:
            backup = Path(backup_path)
            if not backup.exists():
                return {"error": "Backup not found"}

            # Restore files from backup
            for file_path in backup.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(backup)
                    dest = Path(".") / relative_path
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest)

            logger.info(f"âœ… Rolled back to backup: {backup_path}")
            return {"status": "success", "backup": str(backup_path)}

        except Exception as e:
            logger.error(f"Rollback failed: {e}")
            return {"error": str(e)}

    def _get_all_code_files(self) -> List[str]:
        """Get all code files in project"""
        files = []
        extensions = {".py", ".js", ".ts", ".tsx", ".html", ".css"}

        for ext in extensions:
            files.extend([str(f) for f in Path(".").rglob(f"*{ext}")])

        # Filter out ignored directories
        files = [
            f
            for f in files
            if "node_modules" not in f and ".git" not in f and "__pycache__" not in f
        ]

        return files

    async def get_fix_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get recent fix history"""
        return self.fix_history[-limit:]

# Create instance for export
fixer_agent = FixerAgentV2()
