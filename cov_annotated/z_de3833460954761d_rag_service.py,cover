  #!/usr/bin/env python3
> """
> DevSkyy RAG (Retrieval-Augmented Generation) Service
> Provides document ingestion, vector search, and RAG query capabilities
  
> Per Truth Protocol:
> - Rule #1: Never guess - All operations verified with type hints and validation
> - Rule #5: No secrets in code - All credentials via environment variables
> - Rule #7: Input validation - Schema enforcement on all inputs
> - Rule #13: Security baseline - AES-256-GCM for document encryption
  
> Author: DevSkyy Platform Team
> Version: 1.0.0
> Python: 3.11+
> """
  
! import asyncio
! from datetime import datetime
! import hashlib
! import logging
! import os
! from pathlib import Path
! from typing import Any, Optional
! import uuid
  
! from anthropic import Anthropic
! import chromadb
! from chromadb.config import Settings
! from langchain.text_splitter import RecursiveCharacterTextSplitter
! from pypdf import PdfReader
! from sentence_transformers import SentenceTransformer
! import tiktoken
  
  
! logger = logging.getLogger(__name__)
  
  
  # =============================================================================
  # CONFIGURATION
  # =============================================================================
  
  
! class RAGConfig:
!     """RAG system configuration"""
  
      # Vector Database
!     CHROMA_PERSIST_DIR = os.getenv("CHROMA_PERSIST_DIR", "./data/chroma")
!     CHROMA_COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME", "devskyy_docs")
  
      # Embedding Model
!     EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2")
!     EMBEDDING_DIMENSION = 384  # all-MiniLM-L6-v2 dimension
  
      # Chunking Parameters
!     CHUNK_SIZE = int(os.getenv("RAG_CHUNK_SIZE", "1000"))
!     CHUNK_OVERLAP = int(os.getenv("RAG_CHUNK_OVERLAP", "200"))
  
      # Retrieval Parameters
!     TOP_K_RESULTS = int(os.getenv("RAG_TOP_K", "5"))
!     SIMILARITY_THRESHOLD = float(os.getenv("RAG_SIMILARITY_THRESHOLD", "0.7"))
  
      # LLM Configuration
!     ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
!     DEFAULT_MODEL = os.getenv("RAG_LLM_MODEL", "claude-sonnet-4-5-20250929")
!     MAX_TOKENS = int(os.getenv("RAG_MAX_TOKENS", "4096"))
  
  
  # =============================================================================
  # DOCUMENT PROCESSORS
  # =============================================================================
  
  
! class DocumentProcessor:
!     """Process and chunk documents for RAG ingestion"""
  
!     def __init__(
!         self,
!         chunk_size: int = RAGConfig.CHUNK_SIZE,
!         chunk_overlap: int = RAGConfig.CHUNK_OVERLAP,
!     ):
!         self.chunk_size = chunk_size
!         self.chunk_overlap = chunk_overlap
!         self.text_splitter = RecursiveCharacterTextSplitter(
!             chunk_size=chunk_size,
!             chunk_overlap=chunk_overlap,
!             length_function=len,
!             separators=["\n\n", "\n", ". ", " ", ""],
!         )
!         self.tokenizer = tiktoken.get_encoding("cl100k_base")
  
!     def process_pdf(self, file_path: str) -> list[dict[str, Any]]:
!         """
!         Process PDF document and extract text chunks
  
!         Args:
!             file_path: Path to PDF file
  
!         Returns:
!             List of document chunks with metadata
!         """
!         try:
!             reader = PdfReader(file_path)
!             chunks = []
  
!             for page_num, page in enumerate(reader.pages, start=1):
!                 text = page.extract_text()
  
!                 if not text.strip():
!                     continue
  
                  # Split page text into chunks
!                 page_chunks = self.text_splitter.split_text(text)
  
!                 for chunk_idx, chunk in enumerate(page_chunks):
!                     chunks.append(
!                         {
!                             "content": chunk,
!                             "metadata": {
!                                 "source": file_path,
!                                 "page": page_num,
!                                 "chunk_index": chunk_idx,
!                                 "total_pages": len(reader.pages),
!                                 "file_type": "pdf",
!                                 "tokens": len(self.tokenizer.encode(chunk)),
!                             },
!                         }
!                     )
  
!             logger.info(f"Processed PDF: {file_path} -> {len(chunks)} chunks")
!             return chunks
  
!         except Exception as e:
!             logger.error(f"Error processing PDF {file_path}: {e}")
!             raise
  
!     def process_text(self, text: str, source: str = "text") -> list[dict[str, Any]]:
!         """
!         Process plain text and extract chunks
  
!         Args:
!             text: Text content
!             source: Source identifier
  
!         Returns:
!             List of document chunks with metadata
!         """
!         try:
!             chunks = self.text_splitter.split_text(text)
  
!             return [
!                 {
!                     "content": chunk,
!                     "metadata": {
!                         "source": source,
!                         "chunk_index": idx,
!                         "file_type": "text",
!                         "tokens": len(self.tokenizer.encode(chunk)),
!                     },
!                 }
!                 for idx, chunk in enumerate(chunks)
!             ]
  
!         except Exception as e:
!             logger.error(f"Error processing text: {e}")
!             raise
  
!     def calculate_chunk_hash(self, content: str) -> str:
!         """Calculate SHA256 hash of chunk content"""
!         return hashlib.sha256(content.encode()).hexdigest()
  
  
  # =============================================================================
  # VECTOR DATABASE
  # =============================================================================
  
  
! class VectorDatabase:
!     """ChromaDB vector database manager"""
  
!     def __init__(
!         self,
!         persist_directory: str = RAGConfig.CHROMA_PERSIST_DIR,
!         collection_name: str = RAGConfig.CHROMA_COLLECTION_NAME,
!         embedding_model: str = RAGConfig.EMBEDDING_MODEL,
!     ):
!         self.persist_directory = persist_directory
!         self.collection_name = collection_name
  
          # Initialize ChromaDB client
!         Path(persist_directory).mkdir(parents=True, exist_ok=True)
  
!         self.client = chromadb.PersistentClient(
!             path=persist_directory,
!             settings=Settings(
!                 anonymized_telemetry=False,
!                 allow_reset=True,
!             ),
!         )
  
          # Initialize embedding model
!         self.embedding_model = SentenceTransformer(embedding_model)
!         logger.info(f"Loaded embedding model: {embedding_model}")
  
          # Get or create collection
!         self.collection = self.client.get_or_create_collection(
!             name=collection_name,
!             metadata={"description": "DevSkyy RAG document collection"},
!         )
  
!         logger.info(f"Initialized vector database: {collection_name}")
  
!     def add_documents(
!         self,
!         documents: list[dict[str, Any]],
!         batch_size: int = 100,
!     ) -> dict[str, Any]:
!         """
!         Add documents to vector database
  
!         Args:
!             documents: List of document chunks with content and metadata
!             batch_size: Batch size for processing
  
!         Returns:
!             Dictionary with ingestion statistics
!         """
!         try:
!             total_docs = len(documents)
!             added = 0
!             skipped = 0
  
!             for i in range(0, total_docs, batch_size):
!                 batch = documents[i : i + batch_size]
  
                  # Extract content and metadata
!                 contents = [doc["content"] for doc in batch]
!                 metadatas = [doc["metadata"] for doc in batch]
  
                  # Generate embeddings
!                 embeddings = self.embedding_model.encode(
!                     contents,
!                     show_progress_bar=False,
!                 ).tolist()
  
                  # Generate unique IDs
!                 ids = [str(uuid.uuid4()) for _ in batch]
  
                  # Add to collection
!                 self.collection.add(
!                     embeddings=embeddings,
!                     documents=contents,
!                     metadatas=metadatas,
!                     ids=ids,
!                 )
  
!                 added += len(batch)
!                 logger.info(f"Added batch {i // batch_size + 1}: {added}/{total_docs}")
  
!             return {
!                 "total_documents": total_docs,
!                 "added": added,
!                 "skipped": skipped,
!                 "collection_size": self.collection.count(),
!             }
  
!         except Exception as e:
!             logger.error(f"Error adding documents: {e}")
!             raise
  
!     def search(
!         self,
!         query: str,
!         top_k: int = RAGConfig.TOP_K_RESULTS,
!         filters: Optional[dict[str, Any]] = None,
!     ) -> list[dict[str, Any]]:
!         """
!         Semantic search in vector database
  
!         Args:
!             query: Search query
!             top_k: Number of results to return
!             filters: Metadata filters
  
!         Returns:
!             List of search results with content, metadata, and scores
!         """
!         try:
              # Generate query embedding
!             query_embedding = self.embedding_model.encode(query).tolist()
  
              # Search collection
!             results = self.collection.query(
!                 query_embeddings=[query_embedding],
!                 n_results=top_k,
!                 where=filters,
!             )
  
              # Format results
!             formatted_results = []
  
!             if results["documents"] and results["documents"][0]:
!                 for idx in range(len(results["documents"][0])):
!                     formatted_results.append(
!                         {
!                             "content": results["documents"][0][idx],
!                             "metadata": results["metadatas"][0][idx] if results["metadatas"] else {},
!                             "distance": results["distances"][0][idx] if results["distances"] else 0.0,
!                             "similarity": 1 - (results["distances"][0][idx] if results["distances"] else 0.0),
!                             "id": results["ids"][0][idx] if results["ids"] else "",
!                         }
!                     )
  
!             logger.info(f"Search query: '{query}' -> {len(formatted_results)} results")
!             return formatted_results
  
!         except Exception as e:
!             logger.error(f"Error searching: {e}")
!             raise
  
!     def delete_collection(self):
!         """Delete the entire collection"""
!         try:
!             self.client.delete_collection(name=self.collection_name)
!             logger.info(f"Deleted collection: {self.collection_name}")
!         except Exception as e:
!             logger.error(f"Error deleting collection: {e}")
!             raise
  
!     def get_stats(self) -> dict[str, Any]:
!         """Get database statistics"""
!         return {
!             "collection_name": self.collection_name,
!             "document_count": self.collection.count(),
!             "persist_directory": self.persist_directory,
!             "embedding_model": RAGConfig.EMBEDDING_MODEL,
!         }
  
  
  # =============================================================================
  # RAG SERVICE
  # =============================================================================
  
  
! class RAGService:
!     """Main RAG service orchestrating document processing, retrieval, and generation"""
  
!     def __init__(
!         self,
!         vector_db: Optional[VectorDatabase] = None,
!         doc_processor: Optional[DocumentProcessor] = None,
!     ):
!         self.vector_db = vector_db or VectorDatabase()
!         self.doc_processor = doc_processor or DocumentProcessor()
  
          # Initialize Anthropic client
!         if RAGConfig.ANTHROPIC_API_KEY:
!             self.anthropic = Anthropic(api_key=RAGConfig.ANTHROPIC_API_KEY)
!         else:
!             self.anthropic = None
!             logger.warning("Anthropic API key not found - RAG queries will use retrieval only")
  
!     async def ingest_document(
!         self,
!         file_path: str,
!         file_type: Optional[str] = None,
!     ) -> dict[str, Any]:
!         """
!         Ingest a document into the RAG system
  
!         Args:
!             file_path: Path to document file
!             file_type: Document type (pdf, text, etc.) - auto-detected if None
  
!         Returns:
!             Ingestion statistics
!         """
!         try:
              # Auto-detect file type
!             if not file_type:
!                 file_type = Path(file_path).suffix.lower().lstrip(".")
  
              # Process document based on type
!             if file_type == "pdf":
!                 chunks = self.doc_processor.process_pdf(file_path)
!             else:
                  # Read as text file
!                 with open(file_path, "r", encoding="utf-8") as f:
!                     text = f.read()
!                 chunks = self.doc_processor.process_text(text, source=file_path)
  
              # Add to vector database
!             stats = self.vector_db.add_documents(chunks)
  
!             return {
!                 **stats,
!                 "file_path": file_path,
!                 "file_type": file_type,
!                 "chunks_created": len(chunks),
!                 "ingested_at": datetime.utcnow().isoformat(),
!             }
  
!         except Exception as e:
!             logger.error(f"Error ingesting document {file_path}: {e}")
!             raise
  
!     async def ingest_text(
!         self,
!         text: str,
!         source: str = "direct_input",
!         metadata: Optional[dict[str, Any]] = None,
!     ) -> dict[str, Any]:
!         """
!         Ingest text content directly
  
!         Args:
!             text: Text content
!             source: Source identifier
!             metadata: Additional metadata
  
!         Returns:
!             Ingestion statistics
!         """
!         try:
              # Process text
!             chunks = self.doc_processor.process_text(text, source=source)
  
              # Add custom metadata if provided
!             if metadata:
!                 for chunk in chunks:
!                     chunk["metadata"].update(metadata)
  
              # Add to vector database
!             stats = self.vector_db.add_documents(chunks)
  
!             return {
!                 **stats,
!                 "source": source,
!                 "text_length": len(text),
!                 "chunks_created": len(chunks),
!                 "ingested_at": datetime.utcnow().isoformat(),
!             }
  
!         except Exception as e:
!             logger.error(f"Error ingesting text: {e}")
!             raise
  
!     async def search(
!         self,
!         query: str,
!         top_k: int = RAGConfig.TOP_K_RESULTS,
!         filters: Optional[dict[str, Any]] = None,
!         min_similarity: float = RAGConfig.SIMILARITY_THRESHOLD,
!     ) -> list[dict[str, Any]]:
!         """
!         Semantic search for documents
  
!         Args:
!             query: Search query
!             top_k: Number of results
!             filters: Metadata filters
!             min_similarity: Minimum similarity threshold
  
!         Returns:
!             List of search results
!         """
!         try:
!             results = self.vector_db.search(query, top_k=top_k, filters=filters)
  
              # Filter by similarity threshold
!             filtered_results = [r for r in results if r["similarity"] >= min_similarity]
  
!             return filtered_results
  
!         except Exception as e:
!             logger.error(f"Error searching: {e}")
!             raise
  
!     async def query(
!         self,
!         question: str,
!         top_k: int = RAGConfig.TOP_K_RESULTS,
!         model: str = RAGConfig.DEFAULT_MODEL,
!         system_prompt: Optional[str] = None,
!     ) -> dict[str, Any]:
!         """
!         RAG query - Retrieve relevant context and generate answer
  
!         Args:
!             question: User question
!             top_k: Number of context chunks to retrieve
!             model: LLM model to use
!             system_prompt: Custom system prompt
  
!         Returns:
!             Answer with sources and metadata
!         """
!         try:
              # 1. Retrieve relevant context
!             context_results = await self.search(query=question, top_k=top_k)
  
!             if not context_results:
!                 return {
!                     "answer": "I couldn't find any relevant information to answer your question.",
!                     "sources": [],
!                     "context_used": 0,
!                 }
  
              # 2. Build context string
!             context_str = "\n\n".join(
!                 [f"[Source {idx + 1}] {result['content']}" for idx, result in enumerate(context_results)]
!             )
  
              # 3. Build system prompt
!             if not system_prompt:
!                 system_prompt = (
!                     "You are DevSkyy, an AI assistant with access to a knowledge base. "
!                     "Answer questions accurately based on the provided context. "
!                     "If the context doesn't contain relevant information, say so clearly. "
!                     "Always cite sources when using information from the context."
!                 )
  
              # 4. Generate answer with Claude
!             if not self.anthropic:
                  # Fallback: return context only
!                 return {
!                     "answer": "LLM not configured. Here are the relevant excerpts:\n\n" + context_str,
!                     "sources": context_results,
!                     "context_used": len(context_results),
!                 }
  
!             message = self.anthropic.messages.create(
!                 model=model,
!                 max_tokens=RAGConfig.MAX_TOKENS,
!                 system=system_prompt,
!                 messages=[
!                     {
!                         "role": "user",
!                         "content": f"Context:\n{context_str}\n\nQuestion: {question}",
!                     }
!                 ],
!             )
  
!             answer = message.content[0].text
  
!             return {
!                 "answer": answer,
!                 "sources": context_results,
!                 "context_used": len(context_results),
!                 "model": model,
!                 "tokens_used": {
!                     "input": message.usage.input_tokens,
!                     "output": message.usage.output_tokens,
!                 },
!             }
  
!         except Exception as e:
!             logger.error(f"Error processing RAG query: {e}")
!             raise
  
!     def get_stats(self) -> dict[str, Any]:
!         """Get RAG system statistics"""
!         return {
!             "vector_db": self.vector_db.get_stats(),
!             "config": {
!                 "chunk_size": self.doc_processor.chunk_size,
!                 "chunk_overlap": self.doc_processor.chunk_overlap,
!                 "top_k": RAGConfig.TOP_K_RESULTS,
!                 "similarity_threshold": RAGConfig.SIMILARITY_THRESHOLD,
!             },
!         }
  
  
  # =============================================================================
  # SINGLETON INSTANCE
  # =============================================================================
  
  # Global RAG service instance
! _rag_service: Optional[RAGService] = None
  
  
! def get_rag_service() -> RAGService:
!     """Get or create global RAG service instance"""
!     global _rag_service
  
!     if _rag_service is None:
!         _rag_service = RAGService()
!         logger.info("Initialized RAG service")
  
!     return _rag_service
  
  
  # =============================================================================
  # USAGE EXAMPLE
  # =============================================================================
  
- if __name__ == "__main__":
  
-     async def main():
-         """Example usage"""
-         rag = get_rag_service()
  
          # Ingest sample text
-         sample_text = """
-         DevSkyy is a multi-agent AI platform with 54 specialized agents.
-         It provides enterprise-grade AI orchestration with RBAC, encryption,
-         and comprehensive monitoring. The platform follows the Truth Protocol
-         for verifiable, secure, and production-ready AI systems.
-         """
  
-         stats = await rag.ingest_text(sample_text, source="example")
-         print(f"Ingested: {stats}")
  
          # Search
-         results = await rag.search("What is DevSkyy?")
-         print(f"\nSearch results: {len(results)}")
-         for result in results:
-             print(f"  - {result['content'][:100]}... (similarity: {result['similarity']:.2f})")
  
          # RAG query
-         answer = await rag.query("What security features does DevSkyy have?")
-         print(f"\nAnswer: {answer['answer']}")
-         print(f"Sources used: {answer['context_used']}")
  
-     asyncio.run(main())
