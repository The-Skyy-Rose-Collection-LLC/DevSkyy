{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåπ SkyyRose LoRA Training\n",
    "\n",
    "Train a custom LoRA on **604 exact product images** for precise generation.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU (T4 free tier works)\n",
    "- HuggingFace account for saving the model\n",
    "\n",
    "**Runtime:** ~2-3 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q diffusers[torch] transformers accelerate peft bitsandbytes\n",
    "!pip install -q huggingface_hub safetensors pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (required to save model)\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Download the optimized training zip from HuggingFace\n",
    "DATASET_URL = \"https://huggingface.co/datasets/damBruh/skyyrose-lora-dataset-v3/resolve/main/training/skyyrose_training_data.zip\"\n",
    "DATA_DIR = Path(\"/content/training_data\")\n",
    "\n",
    "print(\"üì• Downloading training data...\")\n",
    "!wget -q -O /content/training_data.zip \"{DATASET_URL}\"\n",
    "\n",
    "# Extract\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "with zipfile.ZipFile(\"/content/training_data.zip\", \"r\") as zf:\n",
    "    zf.extractall(DATA_DIR)\n",
    "\n",
    "# Count images\n",
    "images = list(DATA_DIR.glob(\"*.jpg\")) + list(DATA_DIR.glob(\"*.png\"))\n",
    "print(f\"‚úÖ Extracted {len(images)} training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class SkyyRoseDataset(Dataset):\n",
    "    \"\"\"Dataset for SkyyRose LoRA training.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: Path, resolution: int = 512):\n",
    "        self.resolution = resolution\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "\n",
    "        # Find all images and their caption files\n",
    "        for img_path in sorted(data_dir.glob(\"*.jpg\")):\n",
    "            caption_path = img_path.with_suffix(\".txt\")\n",
    "            if caption_path.exists():\n",
    "                caption = caption_path.read_text().strip()\n",
    "            else:\n",
    "                caption = \"skyyrose luxury streetwear product\"\n",
    "\n",
    "            self.images.append(img_path)\n",
    "            self.captions.append(caption)\n",
    "\n",
    "        self.transform = T.Compose(\n",
    "            [\n",
    "                T.Resize((resolution, resolution), interpolation=T.InterpolationMode.LANCZOS),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.5], [0.5]),  # [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(f\"Loaded {len(self.images)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        return {\n",
    "            \"pixel_values\": self.transform(image),\n",
    "            \"caption\": self.captions[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = SkyyRoseDataset(DATA_DIR, resolution=512)\n",
    "print(f\"Sample caption: {dataset.captions[0][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load SDXL Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "\n",
    "print(f\"üîß Loading SDXL on {device}...\")\n",
    "\n",
    "# Tokenizer and text encoder\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    MODEL_ID, subfolder=\"text_encoder\", torch_dtype=dtype\n",
    ").to(device)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "# VAE\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "# UNet (this is what we'll train LoRA on)\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\", torch_dtype=dtype)\n",
    "\n",
    "print(\"‚úÖ Base model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "# Apply LoRA to UNet\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet = unet.to(device)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "print(\"‚úÖ LoRA configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1  # Keep at 1 for T4 GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "SAVE_EVERY = 2  # Save checkpoint every N epochs\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"/content/skyyrose-lora-v3\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "print(\"üìä Training config:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {len(dataloader) * EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"üöÄ Starting SkyyRose LoRA Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "training_log = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    unet.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    progress = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    for batch in progress:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device, dtype=dtype)\n",
    "        captions = batch[\"caption\"]\n",
    "\n",
    "        # Encode text\n",
    "        text_inputs = tokenizer(\n",
    "            captions,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeds = text_encoder(text_inputs.input_ids.to(device))[0]\n",
    "\n",
    "        # Encode images to latents\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Add noise (diffusion forward process)\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device)\n",
    "\n",
    "        # Simplified noise schedule\n",
    "        alpha_t = 1 - (timesteps.float() / 1000).view(-1, 1, 1, 1)\n",
    "        noisy_latents = alpha_t.sqrt() * latents + (1 - alpha_t).sqrt() * noise\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeds).sample\n",
    "\n",
    "        # MSE loss\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Epoch stats\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    training_log.append({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        unet.save_pretrained(OUTPUT_DIR / \"best\")\n",
    "        print(f\"  üíæ Saved best checkpoint (loss: {best_loss:.4f})\")\n",
    "\n",
    "    # Periodic save\n",
    "    if (epoch + 1) % SAVE_EVERY == 0:\n",
    "        unet.save_pretrained(OUTPUT_DIR / f\"checkpoint-{epoch + 1}\")\n",
    "\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save final model\n",
    "print(\"\\nüíæ Saving final model...\")\n",
    "unet.save_pretrained(OUTPUT_DIR / \"final\")\n",
    "\n",
    "# Save training log\n",
    "with open(OUTPUT_DIR / \"training_log.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"trigger_word\": \"skyyrose\",\n",
    "            \"dataset\": \"damBruh/skyyrose-lora-dataset-v3\",\n",
    "            \"images\": len(dataset),\n",
    "            \"log\": training_log,\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "REPO_ID = \"damBruh/skyyrose-lora-v3\"  # Change to your username/repo\n",
    "\n",
    "print(f\"üì§ Uploading to HuggingFace: {REPO_ID}...\")\n",
    "\n",
    "# Create repo if needed\n",
    "try:\n",
    "    create_repo(REPO_ID, exist_ok=True, repo_type=\"model\")\n",
    "except Exception as e:\n",
    "    print(f\"Repo exists or error: {e}\")\n",
    "\n",
    "# Upload the best checkpoint\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=str(OUTPUT_DIR / \"best\"),\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "# Upload training log\n",
    "api.upload_file(\n",
    "    path_or_fileobj=str(OUTPUT_DIR / \"training_log.json\"),\n",
    "    path_in_repo=\"training_log.json\",\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Uploaded to: https://huggingface.co/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "print(\"üé® Testing generation with trained LoRA...\")\n",
    "\n",
    "# Load pipeline\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(device)\n",
    "\n",
    "# Load trained LoRA\n",
    "pipe.load_lora_weights(str(OUTPUT_DIR / \"best\"))\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"skyyrose signature collection lavender rose beanie, luxury streetwear, product photo\",\n",
    "    \"skyyrose black_rose collection sherpa jacket, dark elegance, studio lighting\",\n",
    "    \"skyyrose love_hurts collection windbreaker, bold design, professional photo\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"Generating: {prompt[:50]}...\")\n",
    "    image = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "    ).images[0]\n",
    "\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(prompt.split(\",\")[0][:30])\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "    # Save\n",
    "    image.save(OUTPUT_DIR / f\"test_{i + 1}.png\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"test_results.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Test images saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "Your SkyyRose LoRA is trained and uploaded to HuggingFace.\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "pipe.load_lora_weights(\"damBruh/skyyrose-lora-v3\")\n",
    "\n",
    "image = pipe(\"skyyrose luxury streetwear, [your product description]\").images[0]\n",
    "```\n",
    "\n",
    "**Trigger word:** `skyyrose`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
